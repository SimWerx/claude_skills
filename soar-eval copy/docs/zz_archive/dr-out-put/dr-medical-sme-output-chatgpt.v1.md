# Medical SME Review: SOAR Benchmark Validation

## Executive Summary

Overall, the SOAR evaluation framework aligns well with clinical documentation expectations for Medic Copilot. The current benchmarks cover the critical areas of EMS documentation (negations, factual data extraction, protocol-specific fields, etc.) with appropriate focus on semantic fidelity over clinical decision-making. Key gaps identified include ensuring all refusal documentation elements (e.g. base physician contact in high-risk refusals[\[1\]](https://www.dmemsmd.org/sites/default/files/DMEMSMD%20Protocols%20July%202025%20FINAL%202025-07-14.pdf#:~:text=%E2%80%A2%20Confirm%20decision,care%20unless%20standing%20order%20refusal)) are captured and possibly adding checks for allergy/historical info extraction completeness. Thresholds for safety-critical tests should be tightened (e.g. allergy contraindications must score 1.0[\[2\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L14-L22)), and LLM evaluation prompts require minor refinements to fully capture medical nuance (e.g. including "negative for \___" as a negation phrase[\[3\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L34-L41)). These changes will further ensure that Medic Copilot's output preserves all essential information from the narrative, enabling accurate downstream use.

## Benchmark-by-Benchmark Clinical Review

### A-NEG: Negation Handling

**Clinical Assessment**: **Sound** - Covers common EMS negation patterns with minor refinements.  
**Findings**:  
\- **Coverage**: Benchmarks address simple negations ("no chest pain"), complex/implicit negations, and internal contradictions[\[4\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/rubrics/A-NEG.yaml#L16-L24)[\[5\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/rubrics/A-NEG.yaml#L32-L37). This matches clinical needs; e.g., "patient denies chest pain or SOB" should never be charted as positive findings[\[6\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-NEG1.yaml#L28-L36).  
\- **EMS-Specific Phrasing**: The inclusion of single-sentence contradictions with words like "but" or "however" is explicitly tested (A-NEG4)[\[7\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-NEG4.yaml#L28-L35), which covers cases like _"AOx4 but confused"_ (a contradictory statement EMS providers sometimes dictate). Complex hedged language (e.g. "appears unlikely") is also considered via an implicit negation benchmark (A-NEG3).  
\- **Performance**: The rubric sets a passing threshold of 0.85[\[8\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/rubrics/A-NEG.yaml#L11-L19), reflecting the high importance of polarity preservation (critical requirement: negations must never flip meaning[\[9\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/rubrics/A-NEG.yaml#L22-L26)). Given the medicolegal risk of documenting something as present when it was explicitly denied, this emphasis is appropriate.  
**Recommended Changes**:  
\- \[ \] **Expand Negation Lexicon**: Ensure the LLM judge prompt includes more synonyms and EMS idioms for negation (e.g., "negative for **_," "without_** ," "no evidence of \___"). This will capture phrases like "without complaint" or "denies any other symptoms" which are common in EMS narratives (already planned in prompt refinements[\[3\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L34-L41)).  
\- \[ \] **Implicit Negation Examples**: Add test cases for subtle negation like "only mild pain" (implies no severe pain) or "nothing further" statements, if not already covered by A-NEG3. These ensure the model doesn't erroneously create positives from hedged wording.  
\- \[ \] **Threshold Consideration**: Given the critical nature, consider whether the rubric threshold should be even higher (e.g., 0.90) so that only very minor negation interpretation lapses are acceptable. However, since multiple benchmarks (simple, complex, etc.) feed into A-NEG, the current aggregate 0.85 may suffice.

### A-FCT: Fact Extraction

**Clinical Assessment**: **Sound**, with broad coverage of vital signs and diagnostic impression extraction. A minor gap is noted in medication details extraction.  
**Findings**:  
\- **Vitals Accuracy**: Benchmark A-FCT1 checks that vital sign values (BP, HR, RR, SpO₂, EtCO₂, temp) are captured exactly as dictated[\[10\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FCT1.yaml#L28-L36). This is crucial for clinical fidelity - even small transcription errors can be problematic (e.g., "115/92" vs "154/92" BP[\[11\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FCT1.yaml#L38-L41)). The requirement that vital values match the narrative exactly (with only trivial rounding or unit conversion differences allowed[\[12\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/rubrics/A-FCT.yaml#L20-L28)) is appropriate.  
\- **Impression Matching**: Benchmark A-FCT4 ensures the final impression/diagnosis in the structured output matches the case's ground-truth diagnosis[\[13\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FCT4.yaml#L26-L35). It explicitly allows clinically synonymous terms ("acute MI" vs "STEMI") and minor wording differences[\[14\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FCT4.yaml#L32-L40). This avoids false flags when Medic Copilot uses equivalent medical terminology, focusing the test on true mismatches (e.g., output says "anxiety" when ground truth was sepsis[\[15\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FCT4.yaml#L28-L36)).  
\- **Medications & Procedures**: The rubric description mentions medications (dose, route, timing) as a focus[\[16\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/rubrics/A-FCT.yaml#L16-L24), but there isn't a dedicated A-FCT benchmark for medication extraction aside from safety checks. It appears medication extraction errors are primarily caught under **A-SFT** (for unsafe dose/route) or completeness if a medication is entirely missing (though no explicit benchmark for a missing med was found). In practice, if a med is dictated and omitted, that should be scored under factual extraction errors. Vital trends and timing (pre/post changes) are covered under A-TMP rather than A-FCT.  
**Recommended Changes**:  
\- \[ \] **Medication Extraction Test**: Introduce a benchmark or expand A-FCT1 to explicitly test medication presence (correct drug name, dose, route when dictated). For example, a scenario where narrative: "_gave 0.4 mg nitro SL_" but output misses it should be flagged. This could fall under Fact Extraction rather than Safety if the dose itself was correct but omitted.  
\- \[ \] **Pain Scale and Other Values**: Consider including pain scores and blood glucose in the fact extraction tests. These are often charted alongside vitals. A test case where the narrative reports "_pain 7/10_" or "_BGL 250 mg/dL_" but the output fails to record it would extend coverage to these important patient data points.  
\- \[ \] **No Clinical Ranges**: Continue **not** to enforce normal/abnormal ranges here - as done currently, the eval checks only fidelity to dictated facts (e.g., if HR was said as 150, it must output 150 regardless of it being high). Clinical appropriateness is handled elsewhere; this separation is correctly implemented.

### A-TMP: Temporal Ordering

**Clinical Assessment**: **Sound** - critical sequence and trend logic is tested.  
**Findings**:  
\- **Chronology**: A-TMP1 ensures events in the output occur in the same order as described in the narrative[\[17\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TMP1.yaml#L28-L36). This is clinically important to preserve the timeline of care (e.g., an intubation should be documented after sedation, not before). The benchmark specifically flags outputs where, for example, transport is documented before on-scene interventions, or an intervention is noted without its prerequisite (e.g., "nitroglycerin given" before any BP or ECG evidence of why)[\[17\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TMP1.yaml#L28-L36).  
\- **Trends**: A-TMP2 covers before/after comparisons and trends[\[18\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TMP2.yaml#L26-L34). This catches errors like reversed trends ("pain improved" vs "pain worsened")[\[19\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TMP2.yaml#L36-L39) or omitted follow-up values. This is important for treatments: Medic Copilot must capture that a patient's status changed appropriately after an intervention if stated. The examples cover cases such as pain decreasing after analgesia but the output incorrectly indicating it increased[\[19\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TMP2.yaml#L36-L39).  
\- **Temporal Changes vs Contradictions**: The separation between A-TMP (legitimate sequence of events) and A-NEG4 (internal contradictions) is well-defined. Temporal changes (e.g., vitals improve after fluids) should not be flagged as contradictions, and indeed the contradiction prompt will be refined to ignore such legitimate changes[\[20\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L42-L46).  
**Recommended Changes**:  
\- \[ \] **Temporal Prompt Clarity**: Update the LLM prompt for temporal ordering to explicitly mention common EMS sequence errors: e.g., medications documented before their indication (like pacing documented before stating patient was bradycardic). This can ensure edge cases (like multiple interventions in quick succession) are judged correctly.  
\- \[ \] **Trend Variety**: Add a pediatric vital trend example (e.g., "heart rate dropped from 180 to 160 after intervention" being mis-ordered or misinterpreted) to ensure pediatric scenarios are also covered in A-TMP tests (since children's vitals can change rapidly and must remain coherent in documentation).  
\- \[ \] **Threshold**: Current thresholds (0.85) for A-TMP1/2 seem reasonable[\[21\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TMP1.yaml#L14-L22)[\[22\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TMP2.yaml#L14-L22) given some ambiguity in narrative timing. We do not recommend making these a hard 1.0, as slight timeline discrepancies or narrative ambiguities could otherwise falsely fail a good output.

### A-EVD: Evidence Attribution

**Clinical Assessment**: **Sound**, tests for hallucination or unsupported statements.  
**Findings**:  
\- **Evidence Linkage**: A-EVD1 requires that each structured fact in the output has supporting evidence from the narrative (and that the evidence truly supports the fact)[\[23\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-EVD1.yaml#L26-L34). This is important to ensure Medic Copilot isn't introducing information. For example, if the output impression is "STEMI" but the evidence span only mentions "ST changes" with no explicit STEMI call, that's flagged[\[24\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-EVD1.yaml#L38-L41). Likewise, if draatt_json contains vitals or meds but the evidence map is empty, that's a fail[\[25\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-EVD1.yaml#L36-L44).  
\- **Hallucination Control**: The rubric's relatively high weight (0.50) and proposed threshold increase (from 0.80 to 0.90)[\[26\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L15-L20) reflect the importance of not adding unsupported facts. In documentation, any detail must trace back to what was dictated or observed. The framework correctly treats unsupported output as a serious error (with <0.9 score unacceptable).  
\- **Clinical Context**: This rubric is protocol-agnostic by design - it doesn't check correctness of the fact, only that the fact wasn't hallucinated. This is proper, since Medic Copilot's knowledge base at runtime will handle checking facts against protocols. The eval just ensures no "new" clinical claims appear that weren't said.  
**Recommended Changes**:  
\- \[ \] **Threshold Upgrade**: Enforce the recommended threshold of 0.90 for A-EVD1[\[26\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L15-L20). This tighter threshold means very few unsupported facts are tolerable. Ideally, consider making it even higher for critical fields (e.g., impressions or medications must _always_ have evidence). However, 0.90 overall is a good balance to allow minor mapping issues.  
\- \[ \] **Negative Evidence**: Ensure the LLM prompt covers cases of_mis_attribution as well - e.g., if Medic Copilot links a finding to the wrong part of text (like linking "clear lungs" evidence to the presence of wheezing). While the current concept implies that, an explicit example of incorrect evidence (contradictory span) could be added to the guidelines.  
\- \[ \] **Edge Case**: Document any policy for when a field might appropriately be filled without explicit narrative evidence (perhaps none exists - but if, say, default values or assumptions are never used by Medic Copilot, then all fields should always have evidence). Clarify this in the rubric if needed so SMEs know how to mark cases where something expected wasn't dictated (likely out of scope for eval, but worth noting).

### A-CMP: Completeness

**Clinical Assessment**: **Mostly Sound**, addresses presence of required sections and special fields; one minor gap identified in allergy documentation.  
**Findings**:  
\- **Section Presence**: A-CMP1 through A-CMP4 ensure the six DRAATT sections (Dispatch, Response, Arrival, Assessment, Treatment, Transport) are present and not empty when they should be[\[27\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP1.yaml#L10-L19)[\[28\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP1.yaml#L27-L35). This enforces that Medic Copilot isn't missing an entire section of the report. For example, no case should lack a Dispatch narrative or an Assessment if those were part of the template. A-CMP1 (and related) will fail if a section is entirely absent with content expected[\[29\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP1.yaml#L28-L36).  
\- **DOA/Field Pronouncement**: A-CMP5 specifically targets field pronouncement documentation[\[30\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP5.yaml#L28-L36). Denver protocols require four elements for a pronouncement: method of pronouncement (standing order vs base contact), physician name, exact time, and agency or authority[\[31\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP5.yaml#L10-L18)[\[30\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP5.yaml#L28-L36). The benchmark reflects this: all four must be present for a complete DOA record, with threshold now set to 1.0 (all-or-nothing)[\[32\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP5.yaml#L20-L25)[\[33\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP5.yaml#L14-L19). This aligns with protocol 0050 requirements (e.g., if pronounced by base physician, that physician's name and time must be documented) - any omission is unacceptable[\[30\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP5.yaml#L28-L36).  
\- **Transport/Handoff**: A-CMP6 covers completeness of transport or refusal details[\[34\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP6.yaml#L10-L18)[\[35\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP6.yaml#L28-L36). It checks for elements like transport mode, any delays/incidents en route, who received patient at hospital (handoff), patient condition on arrival, and disposition of patient belongings/equipment[\[35\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP6.yaml#L28-L36). For refusals, it defers to A-REF benchmarks for risk explanation and capacity, but still expects the "disposition" portion (e.g., that the patient refused transport) to be documented. This is important for care continuity and legal documentation of handoff.  
\- **Special Scenario - Air Transfer**: The separate A-CMP6-airport benchmark addresses aeromedical transfer fields[\[36\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP6-airport.yaml#L10-L19)[\[37\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP6-airport.yaml#L28-L36). It requires details like the air agency name, aircraft tail number, exact destination facility, reason for transfer/diagnosis, who had care (flight vs ground crew), etc. These fields come from the "airport transfer" template and reflect industry expectations for interfacility air transports. The benchmark ensures none of these critical transfer data are lost by Medic Copilot.  
\- **Allergies/History**: One area not explicitly seen in benchmarks is general patient history or allergy documentation completeness. For example, if the narrative states "patient has a penicillin allergy" and it's not captured in the output's history/allergies section, there isn't a specific benchmark failing that unless it leads to a med error (A-SFT2 covers only the case where a contraindicated drug was given[\[38\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT2.yaml#L26-L34)). Similarly, past medical history details (e.g. "history of CHF") might be omitted without a direct benchmark. These omissions would slightly lower A-FCT or completeness scores, but no targeted test exists.  
**Recommended Changes**:  
\- \[ \] **Allergy Documentation**: Add a case in **A-CMP6 (or a new A-CMP7)** to ensure stated allergies are captured even if no medication conflict occurs. While clinically it may not cause immediate harm in that call, missing an allergy in the report is a documentation gap. For instance, **Ground Truth**: "_Allergies: Penicillin_"; **Output** lacks it → should be flagged. This could be a low-weight completeness check, as part of the Assessment section completeness.  
\- \[ \] **Mechanism of Injury** (MOI): Consider a trauma scenario check for mechanism documentation. Denver Trauma protocols emphasize mechanism in triage. If the narrative clearly describes a severe MOI (e.g., high-speed MVC rollover) and the output omits it entirely, that could be considered an incomplete Arrival/Dispatch section. Possibly add to A-CMP3 (Arrival completeness) a note to capture MOI details. However, if this is typically caught by overall section presence or by the narrative being embedded in "scene description," a specific benchmark may be unnecessary.  
\- \[ \] **Refusal "No Transport" Field**: Ensure that when a patient refuses, the output explicitly marks a refusal disposition. It appears A-CMP6 covers this ("transport/refusal details incomplete" includes the case of refusal)[\[35\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP6.yaml#L28-L36). The example given ("refusal record with no documented explanation of risks"[\[39\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP6.yaml#L40-L41)) overlaps with A-REF1. No further action needed, just coordinate so A-CMP6 and A-REF aren't double-scoring the same issue.  
\- \[ \] **Hard Gating Critical Fields**: Make A-CMP5 a hard gate (100% required) - this is already implemented (threshold 1.0)[\[40\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP5.yaml#L14-L22). Also consider if A-CMP6-airport critical fields (like destination, etc.) should effectively be near-mandatory (currently 0.85 threshold[\[41\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP6-airport.yaml#L14-L22), which is reasonable given variability in available data).

### A-SFT: Safety Flags

**Clinical Assessment**: **Sound**, with strong focus on "never event" documentation errors. A few extensions are noted for future consideration.  
**Findings**:  
\- **Dose/Route Errors (A-SFT1)**: This benchmark catches when Medic Copilot records an obviously unsafe dose or wrong route for a med[\[42\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT1.yaml#L10-L18). For example, documenting "3240 mg aspirin" instead of 324 mg, or nitroglycerin given IM (an incorrect route)[\[43\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT1.yaml#L36-L40). Such errors, if uncorrected, could propagate dangerous misinformation. The threshold is set to 1.0 (no tolerance for any such error)[\[44\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT1.yaml#L14-L22), which is appropriate - any instance of an output that makes a dose 10x too high or a route dangerously wrong is a failed output. This effectively acts as a **hard gate** for medication safety.  
\- **Contraindications (A-SFT2)**: This test ensures that if a medication or treatment was given despite a contraindication (an allergy or an interacting substance/condition), Medic Copilot flags or acknowledges it[\[38\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT2.yaml#L26-L34). The examples include giving penicillin to a penicillin-allergic patient, aspirin to a known aspirin-allergic patient, or nitroglycerin to a patient on a phosphodiesterase inhibitor (Viagra)[\[45\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT2.yaml#L36-L39). The benchmark assumes that in the narrative ground truth, such conflicts are noted (which they should be), and expects the output to catch it (e.g., via a safety flag or at least documenting "despite allergy"). If it fails to, that's a serious issue. This is also threshold 1.0 now (no missed contraindication is acceptable)[\[46\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT2.yaml#L12-L19)[\[47\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT2.yaml#L21-L25). Denver protocols emphasize checking allergies and contraindications (e.g., nitro is contraindicated if SBP <90 or recent PDE-5 use), so this benchmark covers that logic at the documentation level.  
\- **Post-Intervention Reassessment (A-SFT4)**: This checks that after any high-risk intervention, the medic re-assessed the patient and that Medic Copilot captured that or at least flagged its absence[\[48\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT4.yaml#L9-L17)[\[49\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT4.yaml#L28-L36). High-risk interventions defined here include nitroglycerin, opioids (analgesia), sedatives, paralytics (RSI), intubation, and cardioversion[\[50\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT4.yaml#L10-L18). For example, if morphine was given for pain and the narrative has a later pain score, the output should include it; if the narrative explicitly notes no reassessment was done, the AI should flag the gap[\[49\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT4.yaml#L28-L36). This benchmark is very clinically relevant - failure to re-check vital signs or patient status after interventions is a common documentation issue that can impact patient safety. The threshold is 0.90[\[51\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT4.yaml#L13-L20), meaning a small number of missed reassessments might be tolerated, but generally it expects nearly all to be present.  
\- **System vs Extraction**: It's important that these safety benchmarks test Medic Copilot's inference-time logic more than its transcription. We're evaluating whether the _system_ would catch these issues. For example, Medic Copilot presumably knows standard dose ranges (from protocols) - A-SFT1 tests if it effectively applied that knowledge by not outputting nonsense doses. Similarly, A-SFT2 tests if it would generate a warning or note for a contraindication scenario. This aligns with the scope: these are "system logic" evaluations using ground-truth scenarios.  
**Recommended Changes**:  
\- \[ \] **Vital Sign Contraindications**: In addition to allergies and drug interactions, consider a scenario for vital-sign based contraindications. E.g., narrative: "BP 78/50, yet nitroglycerin was administered," and see if output flags it. Medic Copilot's knowledge (Denver protocol for nitro: contraindicated if SBP <90) should trigger a warning. The working notes mention adding such logic (NTG given with low BP) as a future enhancement[\[52\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L137-L139). We concur - it would strengthen A-SFT2 or be a new case.  
\- \[ \] **Drug Interaction Library**: Ensure the contraindication check covers common prehospital interactions. The PDE-5 inhibitor + Nitro is included[\[45\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT2.yaml#L36-L39). Another example: if the patient was on beta blockers and given high-dose calcium channel blockers causing bradycardia - less likely to be documented in narrative, but any such known interactions should be conceptually covered. The current design is to rely on ground truth flags, which is fine.  
\- \[ \] **Post-Intervention Monitoring**: Possibly broaden A-SFT4 to include **physical restraints** as a high-risk "intervention" that requires reassessment. Denver behavioral protocols (e.g., for restrained patients) call for frequent neurovascular checks. If a scenario describes a patient being restrained, one could expect a subsequent assessment (like "CSM intact in extremities") - omission of that could be flagged. This might overlap with A-BHV1/BHV2, but from a safety perspective it's similar to post-sedation vitals.  
\- \[ \] **Thresholds**: Confirm A-SFT1 and A-SFT2 are treated as **hard gates** (threshold 1.0)[\[2\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L14-L22) - the current YAML shows 1.0 for both[\[44\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT1.yaml#L14-L22)[\[46\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT2.yaml#L12-L19), which is correct. A-SFT4's 0.90 is acceptable as not every scenario will have all vitals (some leeway if one minor follow-up was missed). No changes needed there.

### A-PRT: Protocol Tracking

**Clinical Assessment**: **Sound**, covers major protocol-driven documentation (RSI and STEMI). Could consider expansion for other protocols, but key ones are present.  
**Findings**:  
\- **RSI Intubation (A-PRT1)**: This benchmark verifies that all critical elements of a Rapid Sequence Intubation (advanced airway placement) are documented[\[53\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT1.yaml#L10-L18)[\[54\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT1.yaml#L29-L37). It looks for things like the indication for intubation, pre-oxygenation, the sedative and paralytic drugs/doses, the endotracheal tube size and depth, and confirmation of placement (EtCO₂ value and secondary methods)[\[53\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT1.yaml#L10-L18)[\[54\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT1.yaml#L29-L37). If any of these are missing in the output but were in the narrative, it flags it. For example, if the medic's narrative says they gave ketamine and succinylcholine but the output only shows ketamine, that's a fail[\[55\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT1.yaml#L39-L45). Or if the narrative mentions "ETT 7.5 at 22 cm, confirmed with waveform capnography" and the output lacks the tube size or EtCO₂ value, that's a fail. This aligns perfectly with protocol 1000 (airway management) documentation standards - all these fields are typically required on airway documentation forms. The passing threshold is 0.90[\[56\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT1.yaml#L15-L23), meaning essentially all key fields must be present (allowing perhaps one minor omission).  
\- **STEMI Care (A-PRT2)**: This checks that in acute coronary syndromes (STEMI) cases, all the critical protocol steps were documented or at least flagged[\[57\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT2.yaml#L9-L17)[\[58\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT2.yaml#L27-L35). Specifically, Denver's ACS protocol expects: 12-lead ECG performed (and findings), aspirin given (if no contraindication), nitroglycerin (if indicated and not contraindicated), and activation of a STEMI alert to the hospital when criteria met. The benchmark looks for documentation of those interventions and the alert. For example, if the narrative describes an inferior STEMI and that they gave nitro and started an IV but did **not** give aspirin nor call a STEMI alert, the output should flag those gaps[\[59\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT2.yaml#L36-L40). Medic Copilot should either fill them in if the medic actually did them (in narrative) or explicitly note the deviation if not. The threshold is 0.85[\[60\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT2.yaml#L13-L21), reflecting that missing even one critical step (especially the alert or aspirin) is a serious issue (score would drop to 0.0 if "critical STEMI steps missing"[\[61\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT2.yaml#L22-L25)). This aligns with protocol expectations: documentation of a STEMI case must include that an alert was called and therapies given, or explain why not (e.g., allergy to aspirin, which should then appear as a contraindication note).  
\- **Other Protocols**: The rubrics anticipated future expansion (there are placeholders for stroke, trauma, etc., which indeed we have as separate rubrics A-STR, A-TRM, etc.). So A-PRT is specifically for "protocol branches/paths that aren't already their own rubric." RSI and STEMI were high priority and are addressed. There isn't an A-PRT for "Termination of Resuscitation" because that's covered under A-CAR (cardiac arrest category). Similarly, pediatric dosing is under A-PED. This separation by domain is logical.  
**Recommended Changes**:  
\- \[ \] **Intubation Attempts**: If not already implicitly covered, consider documentation of the number of attempts for intubation. Protocols often require noting if multiple attempts were needed or if another provider took over. If the narrative says "2 attempts needed for intubation," the output should include that in some form. Currently, A-PRT1 doesn't explicitly mention attempts. We could add a note or example to ensure Medic Copilot doesn't omit that detail if present (though it might be included as part of "critical elements" broadly).  
\- \[ \] **Other Protocols to Track**: Evaluate if there are other protocol-driven processes to add under A-PRT. Two possibilities: **Sepsis care** (e.g., did they document recognition and possibly an alert or fluids started early?) - however sepsis documentation might not have a clear singular trigger like STEMI/stroke alerts in prehospital. Another is **Stroke** - but that has its own rubric A-STR. **Trauma alert activation**: Some systems have "Trauma Alert called" similar to STEMI alerts. If Denver or other protocols expect EMS to call ahead for trauma, missing that in documentation could be critical. We didn't see a specific "trauma alert" benchmark; if desired, that could be a candidate (though trauma documentation focuses more on assessment and interventions).  
\- \[ \] **STEMI ECG Documentation**: Ensure that if the narrative describes the 12-lead ECG findings (e.g., "ST elevations in V2-V4"), the output impression includes STEMI. A-PRT2 already covers failing to reflect the STEMI pathway[\[59\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT2.yaml#L36-L40). Possibly include an example where the evidence of STEMI is present but output didn't mark the impression as STEMI - though that might also be caught by A-FCT4 (impression mismatch) or evidence mapping. In any case, A-PRT2's design is solid - no changes needed beyond maintaining it as protocols update.

### A-FMT: Format Validity

**Clinical Assessment**: **Sound** - purely technical but crucial for any output usability.  
**Findings**:  
\- **JSON Structure**: A-FMT1 verifies that Medic Copilot's output JSON is syntactically valid and conforms to the expected DRAATT schema structure[\[62\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FMT1.yaml#L9-L17)[\[63\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FMT1.yaml#L26-L34). It flags outputs that are not valid JSON, missing core keys, or polluted with extra free-text outside the JSON (for instance, if the LLM returns JSON and then some explanation text - which would break parsing)[\[63\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FMT1.yaml#L26-L34)[\[64\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FMT1.yaml#L36-L39).  
\- **No Content Overlap**: This benchmark doesn't directly concern clinical content, but indirectly it ensures content isn't lost due to formatting issues. For example, it catches if an entire section is missing because the JSON was malformed and that part got dropped. The threshold is set to 1.0 (must be perfectly valid)[\[65\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L15-L19)[\[66\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FMT1.yaml#L13-L21) with good reason - an invalid JSON or missing top-level sections cannot be tolerated in production.  
\- **Examples**: The examples given highlight common failure modes: JSON rendered as a string with trailing text, or JSON missing essential sections like chief_complaint or vitals for a full call[\[67\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FMT1.yaml#L36-L41). These are indeed "hard fail" conditions; the eval harness appropriately will give 0.0 in such cases.  
**Recommended Changes**:  
\- \[ \] **Enforce Hard Gate**: Keep this at 1.0 threshold[\[68\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L16-L18). Any run failing A-FMT1 should block deployment, as decided. No partial credit beyond maybe a 0.5 for "parseable but malformed" (which is already defined in scoring but essentially still a failure state)[\[69\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FMT1.yaml#L20-L28).  
\- \[ \] **Schema Updates**: As the DRAATT schema evolves (e.g., if new required fields are added), update this benchmark's checks accordingly. For instance, if "crew_id" or something became a required field in JSON, A-FMT1 should treat its absence as format invalid.  
\- \[ \] **No Additional Clinical Content**: Ensure that we do **not** accidentally use A-FMT to catch missing clinical fields (that's for completeness or fact extraction). Use this solely for structural validity, which is how it's currently scoped. The exclusion criteria explicitly say not to use it for cases where JSON is valid but a clinical field is empty[\[70\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FMT1.yaml#L30-L38) - that is correct usage.

### A-PED: Pediatric Documentation

**Clinical Assessment**: **Sound**, covers the main pediatric-specific risks.  
**Findings**:  
\- **Weight-Based Dosages (A-PED1)**: This is a critical patient safety test. It checks if Medic Copilot correctly handles weight-based dosing calculations[\[71\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PED1.yaml#L8-L16). For example, if the protocol dose is 0.01 mg/kg for a drug and the child's weight is 20 kg, the correct dose is 0.2 mg - the benchmark will fail the output if it recorded something like 0.01 mg or 2.0 mg[\[71\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PED1.yaml#L8-L16)[\[72\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PED1.yaml#L34-L40). In practice, Medic Copilot presumably has a calculator for pediatric doses; this test uses ground-truth "expected dose" vs output. The threshold is 0.95 (effectively a hard requirement)[\[73\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PED1.yaml#L13-L20) and marked as a hard gate[\[74\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PED1.yaml#L14-L17) - appropriate since a miscalculated ped dose is a never-event. This aligns with pediatric protocol emphasis that doses must be in the correct range for weight (e.g., epi dose 0.01 mg/kg). The examples given match common errors: moving a decimal or using an adult dose on a child[\[75\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PED1.yaml#L36-L40).  
\- **Pediatric Completeness (A-PED2)**: This ensures all pediatric-specific assessment info is captured[\[76\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PED2.yaml#L28-L36). That includes the patient's weight (or Broselow tape color/category) if given, APGAR scores for newborns, and special exam findings like capillary refill or fontanelle status if mentioned[\[77\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PED2.yaml#L8-L16)[\[76\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PED2.yaml#L28-L36). Essentially, if the medics provided pediatric context data, the output shouldn't drop it. Examples: a newborn delivery with APGAR 8/9 must have APGAR documented[\[78\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PED2.yaml#L40-L43); if narrative says "Broselow Yellow (≈18 kg)", the output should list weight ~18 kg or at least include that data[\[79\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PED2.yaml#L39-L43). Threshold 0.90[\[80\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PED2.yaml#L14-L22) ensures most of these are present but allows slight omission (perhaps tolerating if one minor detail like cap refill was missed while weight and APGAR were present).  
\- **Protocol Alignment**: Denver pediatric protocols (like dosing in various algorithms) are implicitly covered through these - we're not encoding specific protocol rules (like "max single dose of atropine is X" etc.) in the eval, which is correct. We're simply ensuring that if the crew said a pediatric dose or weight, Medic Copilot didn't flub the math or omit key info. The weight documentation is crucial for downstream calculations (and billing, since weight is needed in pediatric PCRs). APGAR documentation is critical for any newborn delivery (Denver protocols require APGAR at 1 and 5 min if possible).  
**Recommended Changes**:  
\- \[ \] **Additional Pediatric Fields**: Verify if **developmental level** or **caregiver info** needs documentation. For example, noting who gave consent for a minor (parent/guardian) - this might overlap with capacity (if a parent refuses care for a minor, etc., which would fall under refusal policies). If a scenario had a parent on scene, ensure Medic Copilot documents that appropriately in Dispatch or Arrival (this could be a completeness or BHV consideration but related to pediatric context). Currently no explicit pediatric benchmark for that, but it's a minor point.  
\- \[ \] **Normalization**: In A-PED1, clarify in scoring that trivial rounding differences (like calculating 0.195 mg vs output rounding to 0.2 mg) are acceptable[\[81\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PED1.yaml#L32-L40) (the inclusion criteria already exclude minor rounding[\[82\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PED1.yaml#L32-L35)). This is to ensure SMEs don't mistakenly mark a correct rounding as an error. The prompt can emphasize this tolerance.  
\- \[ \] **Weight Units**: Confirm that if narrative gives weight in pounds ("44 lbs child") and output converts to 20 kg, that's considered correct. Possibly add an example to A-PED2 or A-FCT where unit conversion of weight is acceptable, since in the US medics often document in lbs but dosing is in kg. The evaluation should not penalize the AI for converting units correctly.  
\- \[ \] **Hard Gate**: A-PED1 is rightly a hard gate (no errors allowed)[\[74\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PED1.yaml#L14-L17). A-PED2 at 0.90 is fine; missing an APGAR or weight is bad but maybe not deployment-blocking if rare. No change recommended to those thresholds.

### A-CAR: Cardiac Arrest Documentation

**Clinical Assessment**: **Sound**, addresses critical arrest documentation requirements per protocols.  
**Findings**:  
\- **ROSC/Termination Outcome (A-CAR1)**: This benchmark checks that the output clearly documents the outcome of the cardiac arrest - specifically whether Return of Spontaneous Circulation (ROSC) was achieved (and when), or if the resuscitation was terminated in the field (and presumably that the patient was pronounced)[\[83\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CAR1.yaml#L26-L34). Protocol 0051 (Termination of Resuscitation) and 0050 (Field Pronouncement) require that if an arrest is called in the field, the time and authority of termination be recorded, and if ROSC occurs, that's obviously a key event to document. The test ensures that if the narrative says "ROSC at 08:32" or "no ROSC, ceased efforts after 25 minutes," the output reflects that (e.g., with a ROSC timestamp field or a "resuscitation terminated" note)[\[84\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CAR1.yaml#L28-L36)[\[85\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CAR1.yaml#L38-L41). The threshold is 0.95[\[86\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CAR1.yaml#L12-L20) and marked as a hard gate[\[87\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L76-L84) - absolutely appropriate, because failing to indicate whether the patient lived or died (got pulses back or not) is a critical omission in a code report.  
\- **Arrest Event Details (A-CAR2)**: This looks at the granular details of the resuscitation - initial rhythm, number of shocks, medications given during CPR (like epinephrine doses, amiodarone), airway placements, etc.[\[88\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CAR2.yaml#L8-L16)[\[89\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CAR2.yaml#L27-L35). It flags cases where the output provided only a vague summary ("CPR performed") without the specifics that were present in the narrative[\[90\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CAR2.yaml#L28-L36)[\[91\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CAR2.yaml#L36-L41). For example, if the crew's narrative lists "Initial rhythm VFib, shocked x3, epi x2, intubated after second shock" and the output just says "Cardiac arrest treated, CPR done," that's a failure[\[91\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CAR2.yaml#L36-L41). Documenting these details is important for both clinical review and quality improvement - they want to know exactly what was done during the code. The threshold is 0.85[\[92\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CAR2.yaml#L13-L21), meaning most details should be there but perhaps missing one minor item (maybe one medication dose) might still be "Good." This is reasonable; it demands high completeness but recognizes that, say, omitting a single epinephrine dose timestamp might not drop it to zero as long as the key interventions are documented.  
\- **Alignment with Protocol 0051**: The Denver TOR protocol expects that if certain criteria are met (like no ROSC after 30 minutes, etc.), field termination can occur with base contact[\[93\]](https://www.dmemsmd.org/sites/default/files/DMEMSMD%20Protocols%20July%202025%20FINAL%202025-07-14.pdf#:~:text=)[\[94\]](https://www.dmemsmd.org/sites/default/files/DMEMSMD%20Protocols%20July%202025%20FINAL%202025-07-14.pdf#:~:text=complies%20Provide%20care%20per%20protocol). The eval doesn't enforce those rules (appropriately), but by checking documentation of termination time and base physician (which A-CMP5 covers physician name, and A-CAR1 covers outcome), it aligns with documentation expectations. Also, documentation of all interventions (shocks given, meds given, etc.) is aligned with AHA cardiac arrest reporting standards (the UAEMS or Utstein style templates) - A-CAR2 essentially enforces an Utstein-like completeness of the narrative.  
**Recommended Changes**:  
\- \[ \] **Traumatic Cardiac Arrest Nuances**: If not already, ensure scenarios of traumatic arrest vs medical arrest are both covered. For example, a traumatic arrest where no shocks or medications are given (trauma protocol might say no resuscitation if certain conditions) - output should still note the pronouncement. A-CAR1 handles outcome, but A-CAR2 might not apply if no interventions were done to list. This is fine, just ensure the evaluation guidelines note that in some cases (traumatic PEA with obvious death) the "details" might legitimately be few. The exclusion criteria for A-CAR2 do say not to use it if narrative lacks specifics[\[95\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CAR2.yaml#L32-L35), which covers this. Good.  
\- \[ \] **ROSC Timing Field**: If Medic Copilot has a field for ROSC time, ensure it's populated. Possibly tie A-CAR1 scoring to presence of a timestamp or event marker. The benchmark currently says "clearly documented"[\[96\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CAR1.yaml#L22-L30) which implies either a field or a note in narrative. Maybe add an example of acceptable documentation: "output VitalSigns shows a pulse returned at X time, or in narrative 'ROSC achieved at X'" so SMEs know what to look for.  
\- \[ \] **New Consideration - Hospital Alert**: In some systems, if ROSC is achieved, medics call a "ROSC Alert" ahead to the hospital (similar to STEMI/Stroke alerts). Denver doesn't specifically have a "ROSC alert" in protocol, but it might be implicit (if ROSC and post-arrest care, they'd alert receiving). We likely don't need a benchmark for that, but just ensure if a narrative mentioned something like "post-ROSC care, hypothermia initiated" the output captures those interventions (which it would under fact extraction or completeness).  
\- \[ \] **No change to thresholds**: Keep A-CAR1 at 0.95 (effectively 1.0 for outcome presence)[\[87\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L76-L84), and A-CAR2 at 0.85. These were Priority 1 in the plan and appear well-calibrated.

### A-REF: Refusal Documentation

**Clinical Assessment**: **Needs Minor Revision** - covers the most critical aspects (risk explanation and signatures) well, but could explicitly include base contact documentation and patient reasoning.  
**Findings**:  
\- **Risk Explanation (A-REF1)**: This benchmark ensures that if the narrative indicates the medic explained risks of refusal (which is required by protocol)[\[97\]](https://www.dmemsmd.org/sites/default/files/DMEMSMD%20Protocols%20July%202025%20FINAL%202025-07-14.pdf#:~:text=Documentation%20Requirements%20for%20Refusal%20%E2%80%A2,authorizing%20refusal%20of%20care%20unless), the output must include that those risks were explained and/or that the patient acknowledged them[\[98\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/rubrics/A-REF.yaml#L18-L26)[\[99\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/rubrics/A-REF.yaml#L26-L34). In practical terms, Medic Copilot should produce a statement like "Risks of refusing further evaluation/treatment were explained to the patient, who understands and accepts them." If that is missing while the ground truth says it was done, it's a fail. Denver protocol 0032 explicitly lists "Risks of refusal explained to patient" and "Patient understands risks" as documentation requirements[\[97\]](https://www.dmemsmd.org/sites/default/files/DMEMSMD%20Protocols%20July%202025%20FINAL%202025-07-14.pdf#:~:text=Documentation%20Requirements%20for%20Refusal%20%E2%80%A2,authorizing%20refusal%20of%20care%20unless). This addresses those points. The rubric marks that risk explanation is a MUST if stated[\[99\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/rubrics/A-REF.yaml#L26-L34). A-REF1 is thus crucial and is set as a hard gate (recommended threshold 1.0 in planning)[\[100\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L82-L88).  
\- **Capacity & Reasoning**: The rubric focus mentions capacity assessment and patient's reason for refusal[\[98\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/rubrics/A-REF.yaml#L18-L26). However, there is no separate A-REF benchmark for capacity; presumably this is covered by A-BHV2 for decisional capacity if the patient was impaired. If the patient is _not_ impaired (i.e., had capacity), Denver still expects documentation that the patient is alert, oriented, and able to make an informed decision[\[1\]](https://www.dmemsmd.org/sites/default/files/DMEMSMD%20Protocols%20July%202025%20FINAL%202025-07-14.pdf#:~:text=%E2%80%A2%20Confirm%20decision,care%20unless%20standing%20order%20refusal). If the narrative contains such affirmations, Medic Copilot should include them. Currently, A-REF1 might indirectly cover it ("patient understands risks" implies capacity) and A-BHV2 covers cases of _lacking_ capacity. This is mostly sufficient. Similarly, the patient's stated reason for refusal (e.g., "doesn't want to be billed," "prefers to see personal doctor") is mentioned in rubric focus but not explicitly tested. Ideally the output should capture it if the medic documented it. This could be folded into A-REF1's concept (documentation of the discussion).  
\- **Signature/Witness (A-REF2)**: This checks if the refusal signature process was documented[\[98\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/rubrics/A-REF.yaml#L18-L26). E.g., if the narrative says the patient signed an AMA form or a witness (like a police officer or family member) signed because the patient refused to sign, the output should include that information. Denver requires a signed refusal form if possible[\[101\]](https://www.dmemsmd.org/sites/default/files/DMEMSMD%20Protocols%20July%202025%20FINAL%202025-07-14.pdf#:~:text=%E2%80%A2%20Risks%20of%20refusal%20explained,candidate%20for%20an%20alternative%20disposition) - thus, documenting that it was signed (or that patient refused to sign and a witness countersigned) is important. A-REF2 ensures this isn't omitted. Its threshold is 0.85[\[102\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L84-L88), implying we allow slight chance of not capturing it, but ideally it should be there whenever applicable. This is probably fine because sometimes a narrative might not mention the signature explicitly even if it happened (some PCR systems auto-record it). But we want Medic Copilot to mention it if said.  
\- **Base Contact Physician**: A potential gap is documentation of medical control contact for refusals. Protocol says "Name of Base Contact physician authorizing refusal… unless standing order"[\[103\]](https://www.dmemsmd.org/sites/default/files/DMEMSMD%20Protocols%20July%202025%20FINAL%202025-07-14.pdf#:~:text=%E2%80%A2%20Patient%20reminded%20they%20may,candidate%20for%20an%20alternative%20disposition). If the medic had to call a physician (e.g., high-risk refusal), that physician's name should be documented. Our review did not find an explicit mention of this in benchmarks. It could be considered part of "risks explained" or general refusal narrative, but to be sure, we might need a test case. Not documenting base physician when one was involved is a significant medicolegal oversight.  
**Recommended Changes**:  
\- \[ \] **Explicit Base Contact Logging**: Introduce a test (or expand A-REF1) to ensure that if the scenario required base contact, the output includes the physician's involvement. For example, **Ground Truth**: "Contacted Dr. Smith at base, who advised patient of risks and agreed with refusal." If the output omits Dr. Smith, that's a failure. This could be A-REF3 if we want to isolate it. However, it might be sufficiently covered by expecting such info under A-REF1's "risks explained" umbrella. We can simply add to A-REF1's concept: _"…including any base station physician consultation if documented."_ This will align with protocol documentation requirements[\[103\]](https://www.dmemsmd.org/sites/default/files/DMEMSMD%20Protocols%20July%202025%20FINAL%202025-07-14.pdf#:~:text=%E2%80%A2%20Patient%20reminded%20they%20may,candidate%20for%20an%20alternative%20disposition).  
\- \[ \] **Patient's Reason Documented**: Consider adding to A-REF1 or creating A-REF3 (if base contact not used) to check that the patient's stated reason for refusal or circumstances are documented when present. E.g., "patient feels better now," "wants to follow up with PCP," "denies need for ambulance." While not as critical as risk documentation, capturing the patient's perspective is often recommended. The rubric's focus line "Is the patient's reason for refusal captured?"[\[104\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/rubrics/A-REF.yaml#L18-L22) indicates we do intend to include it; making sure Medic Copilot doesn't drop such statements (perhaps via an LLM judge that looks for a reason in the output if one is in narrative) would close this gap.  
\- \[ \] **Thresholds**: Set A-REF1 threshold to 1.0 (hard gate) as planned[\[100\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L82-L88) - missing a risk explanation when one was given is not acceptable. A-REF2's 0.85 is probably okay, since occasionally narratives might not mention signature even if obtained (and we don't want false fails if the data wasn't there to begin with). However, one could argue signature is binary and if the narrative explicitly states it was signed, it should appear (hence could be 1.0 in those cases). It's a minor point; leaving it 0.85 is fine.  
\- \[ \] **Integration with BHV**: Ensure the interplay between refusal and capacity is clear: if a patient lacked capacity, it's not truly a "refusal" in the legal sense (that becomes a treated against will scenario). Those cases should fall out of A-REF and into A-BHV (capacity missing or implied consent). Conversely, a patient with capacity refusing - A-BHV2 might note "capacity documented" but it's really A-REF that covers it. It appears well-thought-out: acceptable deviations in BHV rubric even say capacity can be implied by refusal language[\[105\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/rubrics/A-BHV.yaml#L20-L28). So just maintain that logic.

### A-STR: Stroke Documentation

**Clinical Assessment**: **Sound**, thoroughly covers stroke-specific documentation (stroke scale and LKW).  
**Findings**:  
\- **Stroke Scale/Neuro Exam (A-STR1)**: This verifies that if the narrative contains a stroke assessment - whether a named score (CPSS, LAMS, RACE, etc.) or specific neuro findings - the output includes them[\[106\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-STR1.yaml#L28-L36). Stroke scales are critical in prehospital stroke care documentation. For example, if medics note "LAMS 4, right arm weakness and slurred speech," the AI should not condense that to just "stroke suspected" - it must record the scale or findings[\[107\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-STR1.yaml#L8-L16)[\[108\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-STR1.yaml#L38-L41). The benchmark covers both numeric scores and descriptive findings (gaze palsy, neglect, etc.). This ensures no loss of detail that could affect handoff (e.g., hospital stroke teams want the prehospital stroke score and exam findings). Threshold 0.90[\[109\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-STR1.yaml#L14-L22) is appropriate: missing the entire stroke scale is a fail; maybe one minor neuro descriptor could be missed and still scrape by, but generally it expects completeness.  
\- **Last Known Well & Alert (A-STR2)**: This checks for two crucial time-sensitive pieces: the _Last Known Well_ (LKW) time and documentation that a _Stroke Alert_ was called to the receiving hospital[\[110\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-STR2.yaml#L8-L17)[\[111\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-STR2.yaml#L28-L36). LKW is the time the patient was last seen normal - vital for tPA/thrombectomy decisions, and Denver protocols emphasize recording it. If the narrative provides an LKW (e.g., "last seen normal at 14:25"), the output must show that time[\[111\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-STR2.yaml#L28-L36)[\[112\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-STR2.yaml#L38-L41). If the narrative says a stroke alert was activated, the output must include that. The benchmark is a hard gate (0.95 effectively 1.0)[\[113\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-STR2.yaml#L14-L22), which is justified - omitting LKW or failing to mention the alert when it was done would be a serious documentation lapse. The examples demonstrate a narrative with both LKW and alert mentioned, and expecting both in output[\[112\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-STR2.yaml#L38-L41).  
\- **Alignment**: These align with Denver's stroke protocol (4030), which surely requires documenting LKW and performing a stroke scale. In fact, most EMS protocols treat LKW as a required field for stroke runs. So A-STR2 hitting 1.0 threshold on LKW presence is right. The stroke scale is similarly often required on PCRs. Combined, these benchmarks ensure Medic Copilot doesn't just generically say "stroke" but gives the precise data that stroke centers need.  
**Recommended Changes**:  
\- \[ \] **Multiple Stroke Scales**: If a scenario had more than one scale (not likely, but e.g., CPSS then a RACE score), ensure the evaluation would accept documentation of either or both. Likely fine as is - A-STR1 looks for any scale presence. We might add a note in the prompt: "If any stroke scale or neuro exam described, it should appear - if multiple described, all should appear." But this is edge.  
\- \[ \] **Exact Times**: Clarify in guidance that if LKW time is stated as "unknown" in narrative, it's acceptable (and should be documented as unknown rather than omitted entirely)[\[114\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-STR2.yaml#L32-L36). The exclusion criteria already mention not penalizing "unknown LKW" cases[\[114\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-STR2.yaml#L32-L36). The LLM prompt should treat "documented as unknown" as fulfillment (which is clinically true - documenting that it's unknown is still required).  
\- \[ \] **Hospital Notification**: We have stroke alert covered; ensure if narrative said "stroke alert not given because symptoms resolved" or some nuance, the eval would handle it. Likely outside scope - if no alert was given and narrative explains why, that scenario might not be in ground truth for A-STR2. No change needed, just be aware SMEs should not penalize absence of an alert if the narrative itself had none. The inclusion criteria require that narrative mentions an alert[\[111\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-STR2.yaml#L28-L36). Good.  
\- \[ \] **No threshold change**: A-STR1 at 0.90 and A-STR2 at 0.95 (hard gate for LKW/alert) are appropriate. These were set as Priority 2 - we agree with the prioritization; they're vital but slightly lower immediate patient risk than peds or arrest (Priority 1 categories).

### A-TRM: Trauma Documentation

**Clinical Assessment**: **Sound**, targets key trauma exam and interventions.  
**Findings**:  
\- **GCS/Neuro Status (A-TRM1)**: This benchmark ensures that neurological assessment findings for trauma patients (especially level of consciousness or Glasgow Coma Scale) are documented if provided[\[115\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TRM1.yaml#L28-L36). Trauma protocols (8000-series) require documenting GCS for trauma patients, as it's part of triage criteria. If the narrative says "GCS 13 (E4 V3 M6)" or "patient confused, opens eyes to voice, localizes pain," the output must not omit that[\[115\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TRM1.yaml#L28-L36)[\[116\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TRM1.yaml#L38-L41). This is crucial for detecting deterioration or for trauma team activation (GCS < 13 often triggers a higher level response). The threshold 0.90[\[117\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TRM1.yaml#L14-L22) demands that essentially all trauma cases have a GCS or equivalent documented. This is appropriate; missing GCS is a documentation fail (some PCR systems actually hard-require a GCS entry for trauma). The examples cover both numeric and descriptive neuro status[\[116\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TRM1.yaml#L38-L41).  
\- **Critical Interventions (A-TRM2)**: This checks that any life-saving interventions done for trauma are documented[\[118\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TRM2.yaml#L8-L16)[\[119\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TRM2.yaml#L28-L36). Examples: tourniquet applications, wound packing for hemorrhage, needle thoracostomy (chest decompression) for tension pneumothorax, pelvic binders for unstable pelvic fractures, occlusive chest seals for open chest wounds[\[119\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TRM2.yaml#L28-L36). These are often literally life-or-death interventions and must be noted. If the narrative has "tourniquet applied to left leg at 12:00," the output must mention a tourniquet was applied (and ideally when)[\[120\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TRM2.yaml#L38-L41). If it doesn't, that's a fail. Same for decompression etc. Threshold is 0.95[\[121\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TRM2.yaml#L14-L22) (hard gate) - which is correct: missing any of these is a big deal. You would never want an output that didn't mention a tourniquet was used when it was.  
\- **Trauma Protocol Alignment**: Denver's trauma protocols emphasize documenting high-importance findings (GCS, vital signs, mechanism) and interventions (especially hemorrhage control measures). A-TRM1 and 2 align with this. For instance, an identified gap often is failure to note a second neuro exam (like post-intervention GCS), but at least initial GCS is captured by A-TRM1, and A-SFT4 would cover repeated exam after interventions if narrative had it. These benchmarks ensure that major trauma care steps (e.g., "applied TXA" if it were given, though TXA might be considered a med under extraction) aren't dropped.  
**Recommended Changes**:  
\- \[ \] **Trauma Alert Activation**: As noted earlier, consider if EMS calls a "Trauma Alert" (if that's part of local practice) and whether that should be documented. It's not as universal a practice as stroke/STEMI alerts, but many systems do have "Trauma Team Activation" called in by medics. If we encounter scenarios with that, it might be worth a benchmark akin to A-STR2. Currently, no A-TRM3 exists for this. We could potentially add a low-priority benchmark to check: if narrative says "Trauma alert called," output should mention it. This would fill a parallel gap in protocol tracking.  
\- \[ \] **Mechanism Documentation**: While mechanism of injury (MOI) is not explicitly singled out in a benchmark, it's typically part of the narrative dispatch/arrival sections. We might trust A-CMP3 or A-CMP1 to catch if an entire scene description is missing. If needed, one could add a note in A-TRM or completeness to ensure MOI appears. For example, if the narrative begins "Patient fell 20 feet from ladder" and output nowhere mentions a fall or the height, that's lost information. Perhaps add an example under A-CMP3 or A-TRM1 context. Since mechanism often correlates with trauma category being triggered, it's important context, though not a single data point like GCS or tourniquet.  
\- \[ \] **Vital Trends in Trauma**: If a trauma patient had trends (e.g., BP dropping), A-TMP2 covers trend correctness. Just ensure any unique trauma vitals like **Glasgow Coma Scale trending** (improving or worsening GCS) would be considered under either A-TRM1 (if multiple GCS scores, at least one should appear) or A-TMP2 (if explicitly a trend). Likely okay - not adding a separate test.  
\- \[ \] **Pelvic Binder**: The interventions listed in A-TRM2 include pelvic binder[\[122\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TRM2.yaml#L10-L12). Good - pelvic fractures are often forgotten in documentation; this will catch it. No further changes, just keep the list updated if new trauma procedures (e.g., REBOA - unlikely in EMS at the moment) ever came into scope.

### A-OBS: Obstetric/Neonatal Documentation

**Clinical Assessment**: **Sound**, covers OB delivery and pregnancy details well.  
**Findings**:  
\- **Neonatal Status (A-OBS1)**: This benchmark ensures that after a field birth, the condition of the newborn is documented[\[123\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-OBS1.yaml#L8-L16)[\[124\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-OBS1.yaml#L28-L36). That primarily means APGAR scores (at 1 and 5 minutes if available) and any immediate neonatal resuscitation actions (stimulation, oxygen, etc.). If the narrative provides APGAR scores ("APGAR 8 and 9") or describes the baby as "crying, good tone" etc., the output must include that[\[125\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-OBS1.yaml#L9-L12)[\[126\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-OBS1.yaml#L38-L41). Missing an APGAR is a serious documentation omission (APGAR is a standard part of newborn assessment). Likewise, any interventions like suctioning or stimulation should be noted (the example explicitly mentions if those were narrated but output only says "delivery occurred," it fails[\[126\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-OBS1.yaml#L38-L41)). Threshold 0.95[\[127\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-OBS1.yaml#L14-L22) indicates essentially all such info must be present - appropriate, as leaving out baby status is dangerous (receiving hospital needs to know if baby was vigorous or needed intervention).  
\- **Pregnancy Details (A-OBS2)**: This covers documentation of the mother's obstetric details in OB emergencies or deliveries[\[128\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-OBS2.yaml#L8-L16)[\[129\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-OBS2.yaml#L28-L36). Key items: gestational age, Gravida/Para (number of pregnancies/births), multiple gestation (twins, etc.), and any pregnancy-related complications mentioned (e.g., bleeding, hypertension, rupture of membranes time). If the narrative included these, the output should contain them[\[129\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-OBS2.yaml#L28-L36)[\[130\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-OBS2.yaml#L38-L41). For example, if medics note "36 weeks pregnant, G3P2, with vaginal bleeding," the output must include that she's 36 weeks, was G3P2, etc., not just say "pregnant patient"[\[130\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-OBS2.yaml#L38-L41). This is important for context - knowing how far along the pregnancy is changes management. The threshold is 0.90[\[131\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-OBS2.yaml#L14-L22), so mostly complete info expected with slight allowance. Typically, medics are pretty good about at least noting weeks and maybe G/P; the eval expects those to be preserved.  
\- **Protocol Alignment**: Denver OB protocols (7000-series) require certain documentation: if delivery, time of birth, any complications, APGAR, etc. These benchmarks enforce those. Also, "pregnancy details omitted" covers scenarios like trauma in a pregnant patient or other OB emergencies (e.g., eclampsia) where gestational age or complications need noting. The inclusion criteria explicitly mention preeclampsia, bleeding, etc. So if a medic documented "pregnant patient ~35 weeks, signs of preeclampsia (BP 160/100)", the output better include that pregnancy context or it loses critical info.  
**Recommended Changes**:  
\- \[ \] **Birth Time**: If not already included, consider that documenting the time of birth for the newborn is also important. If narrative gives a birth time ("delivered baby at 03:35"), Medic Copilot should capture it (likely in the timeline of events). A-OBS1 doesn't explicitly mention birth timestamp. Possibly it's recorded in the Treatment or narrative chronology. We might ensure this by including a check or example. However, since it's more of a timeline event, it might be implicitly covered by having it in narrative events and expecting it in output chronology (maybe A-TMP handles ordering, but not presence of time). Could add: _"If a delivery occurred, the output should include a delivery timestamp or narrative note of time delivered."_ This is a small detail that could be added to completeness or left to general fidelity.  
\- \[ \] **Post-partum Maternal Status**: We cover neonatal status. What about the mother's post-delivery status? Protocols often require noting if placenta delivered, est. blood loss, perineal tears, etc., but medics might not always have all that. Not crucial for the AI eval unless in narrative. If a scenario had "placenta delivered, bleeding controlled," ideally output includes it. That might just fall under general extraction if mentioned. We don't need a specific benchmark, but SMEs should be mindful to look for such details in OB cases. Possibly add an example to A-OBS2: "Narrative: 'Placenta delivered intact, estimated blood loss ~500 mL' but output omits EBL." This could be another completeness item for OB cases. Not strictly in current benchmarks - maybe not needed unless such scenario is common in test set.  
\- \[ \] **Twin Delivery**: Ensure the logic can handle multiple births. If narrative says "twin delivery," output should reflect two neonates with respective APGARs, etc. A-OBS1 might need to account for multiple sets of neonatal info. Perhaps add a twin scenario as a test case to ensure the AI doesn't collapse them into one or drop one baby's info. This is a specialized case but a possible edge.  
\- \[ \] **No threshold change**: A-OBS1 at 0.95 (hard gate) and A-OBS2 at 0.90 are appropriate. If anything, one could argue A-OBS1 should be 1.0 since missing APGAR is critical. But 0.95 with scoring scale effectively means if APGAR is missing but maybe some baby info present, it might give partial credit - however, since APGAR and status are usually all-or-nothing, likely any omission drags it down. No strong need to adjust.

### A-BHV: Behavioral/Psychiatric Documentation

**Clinical Assessment**: **Sound**, covers high-liability areas of behavioral calls.  
**Findings**:  
\- **Restraint/Sedation Use (A-BHV1)**: This test ensures that if the patient was combative and required restraints (physical or chemical), the output reflects that intervention[\[132\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-BHV1.yaml#L28-L36)[\[133\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-BHV1.yaml#L39-L42). For example, if the narrative says "patient was violent, 4-point soft restraints applied" or "received 5 mg Versed IM for agitation," the AI must document restraints and sedative given[\[134\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-BHV1.yaml#L8-L16)[\[132\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-BHV1.yaml#L28-L36). Not documenting a restraint is a major liability - it would appear as if the patient was not restrained, which could be problematic if they arrived restrained without explanation. Similarly, giving a sedative (like midazolam or haloperidol) without charting it is dangerous. The benchmark weight is high and threshold 0.95[\[135\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-BHV1.yaml#L14-L22), effectively a hard requirement (and indeed marked hard_gate[\[136\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-BHV1.yaml#L14-L17)). That is correct: any omission here is unacceptable. This test overlaps somewhat with medication extraction (for the sedative dose) but is specific to behavioral contexts, which is good to isolate since sometimes sedation might be documented in a special way.  
\- **Capacity/Consent (A-BHV2)**: This checks that when a patient's decision-making capacity or legal consent status is in question, it was documented[\[137\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-BHV2.yaml#L8-L16)[\[138\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-BHV2.yaml#L28-L36). Examples include patients on a psychiatric hold (involuntary), patients deemed not competent due to their mental state, or use of implied consent for an incapacitated patient[\[138\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-BHV2.yaml#L28-L36)[\[139\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-BHV2.yaml#L39-L42). If the narrative says "patient lacks capacity due to intoxication, treated under implied consent" or "placed on 72-hour hold by police," the output needs to capture that. This is legally required documentation whenever we treat a patient against their will or without explicit consent. The threshold is 0.90[\[140\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-BHV2.yaml#L14-L22), so most of these instances must be documented. That's reasonable - occasionally capacity might be borderline and not explicitly stated, but if stated, the AI must not ignore it. The examples show crucial scenarios: 5150 hold mentioned, or "deemed incompetent" mentioned[\[139\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-BHV2.yaml#L39-L42) - these should absolutely appear in the output.  
\- **Alignment**: Denver behavioral guidelines (6000-series and 1130 restraint procedure) emphasize documenting indications for restraint, method used, patient response, and legal considerations (like placing patient on a mental health hold). These benchmarks enforce capturing exactly that information. By treating restraint/sedation omission as a hard fail and capacity documentation as essential, it covers EMS's biggest medicolegal risk on psych calls: if someone later asks "did the medic have the right to treat this patient?" the documentation of capacity or lack thereof is the answer. A-BHV2 ensures that's in the report.  
**Recommended Changes**:  
\- \[ \] **Reassessment after Restraint**: Although outside the direct scope of this benchmark, note that protocols require periodic reassessment of a restrained patient's status (circulation, etc.). Medic Copilot should capture if narrative included such reassessments. This is partly covered by A-SFT4 for any high-risk intervention (restraint could be considered high-risk). We might include physical restraints in the list of "high-risk interventions" to ensure any mention of checking circulation after restraints isn't missed. Alternatively, ensure A-BHV1 doesn't only note the act of restraining but also any associated required monitoring if stated. Perhaps an example or note in A-BHV1: _"If narrative describes post-restraint monitoring (e.g. 'CMS intact'), output should include it."_ However, this might be too granular; likely fine as is.  
\- \[ \] **Distinguish Chemical vs Physical**: In evaluation, ensure that if either a sedative was given or physical restraints applied, the benchmark triggers. A-BHV1's inclusion mentions both scenarios[\[132\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-BHV1.yaml#L28-L36). Just make sure the scoring/prompt accounts for cases where only one type occurred. The current design likely does: any mention of either should result in expecting documentation of that.  
\- \[ \] **Consent for Minors**: One scenario: a minor who is with a parent who refuses care for them (this mixes refusal and behavioral possibly). If a parent refuses transport for a minor, EMS often requires law enforcement or base contact if it's high risk. That scenario might not be directly covered by A-BHV2 (which is more about patient's own capacity) or A-REF (which is about adult patients refusing). However, it is partly covered by protocol (in 0032, minors require a guardian's consent)[\[141\]](https://www.dmemsmd.org/sites/default/files/DMEMSMD%20Protocols%20July%202025%20FINAL%202025-07-14.pdf#:~:text=No%20base%20contact%20required%20if,refer%20to%20patient%20determination%20protocol)[\[142\]](https://www.dmemsmd.org/sites/default/files/DMEMSMD%20Protocols%20July%202025%20FINAL%202025-07-14.pdf#:~:text=Base%20Contact%20Required%20%E2%80%A2%20,making%02capacity%20may%20refuse%20examination). If such a scenario appears, documentation should note parent/guardian consent or refusal. We might consider a benchmark, but it's a rare edge. For now, just flag as a complexity: ensure SMEs verify that if a minor refusal happened, the output documented the guardian aspect. This could be an expansion of A-REF or a new nuance for BHV ("consent by proxy"). Given the complexity, perhaps leave it unless data suggests it's an issue.  
\- \[ \] **Thresholds**: Keep A-BHV1 at effectively 1.0 (0.95 with hard gate)[\[135\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-BHV1.yaml#L14-L22) - that's fine. A-BHV2 at 0.90 is fine; if narrative explicitly notes incapacity and output misses it, that likely scores 0.0 anyway. No changes needed.  
\- \[ \] **Hard Gate Consideration**: One could argue A-BHV1 (restraint documentation) should be absolutely 1.0 with no partial credit, since either you documented the restraint or not. But the scoring scale already makes 0.95 = "key intervention captured" and 0.0 = "omitted"[\[143\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-BHV1.yaml#L22-L27), which is effectively binary in practice (the 0.80 "restraint noted but details missing" case might apply if it said restrained but not how or no dose, etc.). This is acceptable as-is.

## Missing Failure Modes (Proposed New Benchmarks)

Despite comprehensive coverage, a few additional failure modes could be addressed to further improve Medic Copilot's evaluations. Each proposed item below identifies a documentation element, the potential error, and how we could structure a benchmark to catch it.

### Proposed: A-ALL1 (Allergy/Omission)

**Category**: Extraction  
**Documentation Element at Risk**: Patient allergies or medical history mentions in narrative.  
**Failure Mode**: Medic Copilot fails to record a stated allergy or pertinent history because it wasn't directly relevant to an intervention. For example, narrative: "_Known allergy: sulfa drugs._" Output: allergy section empty or missing sulfa. While not immediately safety-critical (if no sulfa given), it's a completeness issue for the record.  
**Ground Truth Structure**: We would include in the test narrative a clear mention of an allergy or history item and expect it in the structured output's allergies or history fields.  
**Suggested Benchmarks**:  
\- **A-ALL1**: _"Allergy Not Documented."_  
**Input**: "Patient reports allergy to penicillin. No penicillin was given during call."  
**Expected extraction**: { allergies: \["Penicillin"\] } in draatt_json.  
**Concept**: If an allergy is stated, it should appear in output even if it didn't come into play. Score 1.0 if captured, 0.0 if omitted.  
_(Note: This could also be a low-weight addition to A-CMP completeness rubric rather than its own rubric since it's a specific completeness issue.)_  
\- **A-HX1** (if extending to med history): _"History Element Omitted."_  
**Input**: "Narrative: 'Past medical history: CHF, hypertension.'"  
**Expected**: output's history section lists CHF and HTN.  
This would catch if Medic Copilot dropped general history details.

### Proposed: A-REF3 (Base Contact Documentation)

**Category**: System Logic / Completeness  
**Documentation Element at Risk**: Base physician consult for high-risk refusal or unusual cases.  
**Failure Mode**: Medic Copilot does not document that the crew contacted medical control when they did, or omits the physician's name/advice. Protocol 0032 requires physician name if base was contacted for refusal[\[103\]](https://www.dmemsmd.org/sites/default/files/DMEMSMD%20Protocols%20July%202025%20FINAL%202025-07-14.pdf#:~:text=%E2%80%A2%20Patient%20reminded%20they%20may,candidate%20for%20an%20alternative%20disposition). Also, many protocols require documenting base contact for orders in certain treatments.  
**Ground Truth Structure**: Include a scenario where base contact is clearly stated. E.g., narrative: "_Contacted base: spoke with Dr. Adams who agrees with field termination_," or "_Base physician Dr. Jones advised to administer additional dose_." Expect output to include that info in an appropriate section (e.g., notes or orders).  
**Suggested Benchmarks**:  
\- **A-REF3**: _"Base Contact Omitted."_ (Could be under A-REF rubric or A-SFT rubric if it's about orders).  
**Input** (refusal example): "Despite high risk, patient refuses transport. Contacted base physician Dr. Smith, who concurred with refusal after risks explained."  
**Expected output**: should mention Dr. Smith's involvement. If output just says "patient refused" with no mention of base, it fails.  
**Input** (treatment example): "Chest pain call, hypotensive. Contacted Dr. Lee for orders - received order to give fluid bolus."  
**Expected**: Output includes something like "Online medical control consulted (Dr. Lee), order received for fluid bolus."  
This ensures continuity of authority and that Medic Copilot captures the chain of command when medics go outside standing orders.  
_(This may overlap with scenario design; it's an important completeness check but might be lower priority if such scenarios are rare in test data.)_

### Proposed: A-TRM3 (Trauma Alert Notification)

**Category**: System Logic / Completeness  
**Documentation Element at Risk**: Prehospital trauma team activation notification.  
**Failure Mode**: Medic Copilot fails to mention that a "Trauma Alert" or similar was called to the receiving hospital when the narrative indicates it was. Many EMS systems call ahead for major trauma. While not universal, if present in narrative, it's as critical as STEMI/Stroke alerts.  
**Ground Truth Structure**: A trauma scenario where medics announce a trauma alert. E.g., narrative: "_Trauma alert called to Medical Center at 14:50 for this patient._" Expect output to contain a statement about trauma alert activation.  
**Suggested Benchmarks**:  
\- **A-TRM3**: _"Trauma Alert Not Documented."_  
**Input**: "18-year-old fall >20 ft with GCS 8. Trauma alert activated, ETA 5 minutes."  
**Expected**: Output includes "Trauma alert" in transport or narrative. If output just describes injuries and transport without mention of the alert, that's a miss.  
This would mirror A-STR2's logic but for trauma. We'd weight it similarly (threshold ~0.95, since an alert is as important to document as it is to perform).

### Proposed: A-BRN1 (Burn Injury Documentation)

**Category**: Extraction  
**Documentation Element at Risk**: Burn injury assessment specifics.  
**Failure Mode**: Medic Copilot might misinterpret or omit burn size/severity information. Burn documentation often includes total body surface area (TBSA) percentage burned and degree of burns, which can be verbally complex (e.g., "approx 30% second-degree burns to torso"). If narrative states these and output doesn't, that's a loss of critical info for triage.  
**Ground Truth Structure**: Burn scenario narrative with explicit percentages or degree.  
**Suggested Benchmark**:  
\- **A-BRN1**: _"Burn Details Omitted/Misreported."_  
**Input**: "Patient with partial thickness burns ~40% TBSA (chest, arms). Third-degree on right arm ~5%. Singed nasal hair present."  
**Expected**: Output notes 40% partial thickness burns, 5% full thickness to arm, inhalation injury signs.  
Failure if output just says "Severe burns" without specifics, or mixes up percentages.  
_(Burn cases are fewer, but if present in synthetic corpus, this would ensure Medic Copilot handles the numeric info right. This might be lower priority compared to others above.)_

These proposed benchmarks are provided for consideration. The most impactful of them are the allergy documentation and base contact documentation, as those patch small but important holes in completeness and medicolegal safety. The trauma alert and burn documentation cases are more specialized and can be prioritized accordingly if those scenario types are in scope.

## Threshold and Weight Recommendations

Based on the criticality of each benchmark's content (and any observed performance issues), I recommend the following threshold and weight adjustments:

| Benchmark | Current Threshold | Recommended | Justification (extraction accuracy basis) |
| --- | --- | --- | --- |
| **A-SFT1** (Dose/Route) | 0.95[\[44\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT1.yaml#L14-L22) | **1.0** (hard gate)[\[44\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT1.yaml#L14-L22) | No unsafe dose/route is acceptable. Even one such error is a "never event"[\[144\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L13-L20). Must score 100% if all meds within safe ranges; any violation = fail. |
| **A-SFT2** (Contraindications) | 0.95[\[46\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT2.yaml#L12-L19) | **1.0** (hard gate)[\[46\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT2.yaml#L12-L19) | Allergy or contraindication oversight can be fatal. If a conflict was stated and MC failed to flag it, that's a critical miss[\[145\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L14-L17). Binary pass/fail is appropriate. |
| **A-FMT1** (JSON Format) | 0.95[\[66\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FMT1.yaml#L13-L21) | **1.0** (hard gate)[\[66\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FMT1.yaml#L13-L21) | Output must be parseable. Any JSON format error renders the output unusable[\[146\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L15-L18). No partial credit - it either parses correctly or not. |
| **A-CMP5** (DOA Pronouncement Fields) | 0.90[\[40\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP5.yaml#L14-L22) | **1.0** (hard gate)[\[32\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP5.yaml#L20-L25) | All four pronouncement elements are mandatory per protocol[\[30\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP5.yaml#L28-L36). Missing even one (e.g., physician name) is an incomplete death documentation. Should require 100%[\[147\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L16-L24). |
| **A-EVD1** (Evidence Support) | 0.80[\[148\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-EVD1.yaml#L14-L21) | **0.90**[\[149\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L18-L20) | Reduce tolerance for unsupported facts. Initially 20% of facts could lack valid evidence; that's too high. Tightening to 90% forces nearly all output facts to be grounded[\[26\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L15-L20), minimizing hallucinations while allowing minor span mismatches. |
| **A-REF1** (Refusal Risks) | 0.95 (implied)[\[150\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/rubrics/A-REF.yaml#L13-L21) | **1.0** (hard gate) | If risks were explained in narrative, they must appear in output[\[99\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/rubrics/A-REF.yaml#L26-L34). This is a medicolegal critical point. We treat it as pass/fail - any omission of documented risk explanation is unacceptable. |
| **A-BHV1** (Restraint/Sedation) | 0.95[\[135\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-BHV1.yaml#L14-L22) | **1.0** (hard gate) | Currently effectively a hard gate (0.95 with hard_gate tag) - recommend making that an explicit 1.0. Omission of a restraint or chemical sedative is a serious liability. It's either documented or not, so binary scoring fits. |
| **A-STR2** (LKW/Stroke Alert) | 0.95[\[113\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-STR2.yaml#L14-L22) | _No change_ (keep 0.95) | Already essentially a hard requirement. LKW time and stroke alert are too critical to miss. 0.95 is fine (since 0.95 is defined as "critical info present" and anything less fails)[\[151\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-STR2.yaml#L22-L26). |
| **A-CAR1** (ROSC/Termination) | 0.95[\[86\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CAR1.yaml#L12-L20) | _No change_ (keep 0.95) | This is a de facto hard gate - failing to document arrest outcome is not allowed. 0.95 is fine to represent 100% needed (similar rationale as above). |
| **A-PED1** (Ped Dose Calc) | 0.95[\[73\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PED1.yaml#L13-L20) | _No change_ (keep 0.95) | Already marked hard_gate. Pediatric dosing errors must be zero. The scoring scale already makes <0.95 a failure[\[152\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PED1.yaml#L20-L25). |

_Table Note:_ Many of the above "0.95" thresholds are effectively treated as 1.0 due to scoring definitions (0.95 is often the highest below 1.0 given for "excellent"). The recommendation to set them explicitly to 1.0 is to make the pass condition unambiguous. For example, A-SFT1 and A-SFT2 now show threshold 1.0 in the YAML after updates[\[44\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT1.yaml#L14-L22)[\[46\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT2.yaml#L12-L19), which is good. Similarly, we will enforce that for others like A-CMP5. The justification is that these are binary pass/fail criteria in practice - either the critical item was documented or it wasn't. Partial scoring doesn't make sense except as an intermediary for aggregated scoring, but since these are likely gating criteria for deployment, making them 1.0 clarifies that any miss triggers a failure of that rubric.

As for weights, most rubric weights (0.125 each in rubric definitions for A- categories) seem balanced, ensuring no single rubric dominates the overall score, yet within each rubric the critical benchmarks have higher weight. The only tweak might be to slightly increase weight of A-SFT and A-REF rubric in the composite score if we consider them higher importance than formatting. However, since gating thresholds handle the "must-pass" nature, weight adjustments aren't strictly necessary for pass/fail decisions. Weights mainly matter for composite quality scoring. The current scheme (equal weights for A- categories, I infer) is acceptable given gating is used for the truly critical items.

## LLM Prompt Improvements

Reviewing the LLM-as-a-judge prompts, they largely need more specificity to avoid false positives/negatives, as identified. Below are recommendations for each prompt file that impacts clinical evaluation accuracy, focusing on extraction fidelity (not advising clinical decisions).

### negation_simple_prompt.md

**Issue**: Lacked comprehensive examples of common negation phrasing. May not explicitly instruct the model to treat "X not present" or "without X" as negations. Also need to ensure that if something was explicitly denied and the output left it as unknown (not just mis-marked positive), that's considered a misinterpretation (the eval should catch non-documentation of a stated negation, not only polarity flips).  
**Suggested Addition**: Add a bullet in guidelines such as: _"Treat any explicit negation in the narrative (e.g., 'denies_ **_', 'negative for_** _', 'without \___') as an indication that the output must mark that finding as absent. If the output instead marks it present_ _or omits it entirely, count that as an error."_[\[3\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L34-L41). Also include example phrasings: "Patient denies chest pain" - output should not list chest pain as a complaint; "No signs of trauma" - output should reflect no trauma noted. Emphasize that "unknown" or blank when a denial was given is a miss (because it failed to preserve the stated absence). This will cover cases where MC didn't flip to 'true' but just didn't document the negation - which is equally an issue. The working notes already indicate adding synonyms like "negative for X, without X, X not present" and the point about unknown vs explicit denial[\[3\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L34-L41).

### negation_complex_prompt.md

**Issue**: May not adequately handle double negatives or contrastive statements. We want the LLM judge to recognize constructions like "**does not deny** chest pain" (which means patient _admits_ chest pain - a tricky double negative) or "**except**" and "**but**" clauses. Without explicit examples, the LLM might not catch these nuances reliably.  
**Suggested Addition**: Incorporate clarifications: _"If narrative uses a double negative (e.g., 'not denying_ _pain'), interpret it correctly (in this example, the patient does have pain - output should reflect pain present). If narrative uses phrases like 'but' or 'except' to introduce an exception after a positive or negative statement, ensure the output respects that nuance (e.g., 'denies chest pain_ _but_ _has mild abdominal pain' - chest pain should be negative, abdominal pain positive)."_ Include an example like: "Narrative: 'Patient doesn't deny headache' -> the patient **has** a headache (not denying = admitting). Flag output as wrong if it marks headache absent." And: "Narrative: 'No trauma noted except a small abrasion' -> output should capture the abrasion despite the initial 'no trauma'." This will guide the LLM judge to handle these edge cases. The working plan to add "but/except" examples and clarify "not denying = present" should be implemented[\[153\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L38-L41).

### contradiction_prompt.md

**Issue**: The initial prompt might flag any change as a contradiction. It needs to differentiate between true contradictions vs temporal changes or evolving patient states. Also might not have included a straightforward "X but Y" example.  
**Suggested Addition**: Emphasize temporal context: _"Ignore sequences that indicate a change over time (improvement or deterioration) - these are not contradictions if plausible (e.g., patient was unresponsive then later alert after treatment). Focus on_ _simultaneous or logically incompatible statements."_[\[154\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-NEG4.yaml#L28-L36) Also explicitly mention single-sentence contradictions: _"If one sentence says two opposing things (e.g., 'patient alert and oriented ×4_ _but_ _confused'), that's a contradiction that should have been flagged."_[\[154\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-NEG4.yaml#L28-L36) Add that as an example in the prompt. The working notes indeed call to add temporal context guidance and a "but" example[\[20\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L42-L46). After revision, the LLM judge will better ignore allowed changes (like vitals improving after fluid) and catch true documentation inconsistencies that Medic Copilot missed flagging.

### vitals_extraction_prompt.md

**Issue**: The LLM might currently flag even tiny numeric differences or unit conversions as errors. We want to allow, for example, Fahrenheit vs Celsius or rounding differences that don't change the clinical meaning. Without clarity, the LLM might consider "37°C vs 98.6°F" a mismatch, or 97% vs 98% SpO₂ a mismatch, which we consider acceptable[\[155\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FCT1.yaml#L32-L36).  
**Suggested Addition**: Include a note: _"Accept minor rounding or equivalent unit conversions as correct. For instance, 98.6°F in narrative vs 37°C in output - that's okay. Or BP 154/92 vs 155/92 due to rounding - not a failure. Only flag if the numeric difference would be considered a clearly wrong value or different patient status."_[\[156\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L46-L50) Provide an example: "Narrative: temp 38.0°C, Output: temp 100.4°F - this is correct (same fever, just converted). Do not penalize." And "Narrative: pulse ox 97%, Output: 98% - trivial difference, no significant error." Conversely, "Narrative: BP 154/92, Output: 115/92 - this is a true error"[\[11\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FCT1.yaml#L38-L41). This guidance will reduce false positives from trivial discrepancies. The working list already notes adding unit conversion tolerance and trivial rounding acceptance[\[156\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L46-L50).

### impression_mismatch_prompt.md

**Issue**: The LLM judge must distinguish between clinically synonymous impressions vs truly different ones. The current prompt might flag an impression as a mismatch even if it's essentially the same diagnosis phrased differently. We want to avoid penalizing "myocardial infarction" vs "heart attack" vs "STEMI" differences, etc. The YAML for A-FCT4's exclusion covers this[\[14\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FCT4.yaml#L32-L40), but the prompt needs to reinforce it.  
**Suggested Addition**: Add explicit examples of synonyms not to flag: _"Treat clinically equivalent terms as a match. E.g., 'STEMI', 'acute MI', and 'heart attack' all refer to the same condition_[_\[157\]_](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FCT4.yaml#L34-L42)_. 'Hyperglycemia' vs 'high blood sugar' - same thing. Do not count these as mismatches."_[\[157\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FCT4.yaml#L34-L42) Also instruct: _"If the output impression is more general but encompasses the specific ground truth diagnosis (or vice versa) and is not incompatible, don't flag it. Only flag if the output's impression is clearly a different diagnosis or misses a critical aspect."_ For example: ground truth "acute appendicitis" vs output "abdominal pain" - that should be flagged because one is specific, the other is vague (output lost specificity). But ground truth "ST-elevation MI" vs output "acute myocardial infarction" - do not flag (essentially same). The working notes to add synonym equivalence examples like MI = heart attack, etc., align with this[\[158\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L50-L54).

### repeat_vitals_prompt.md

**Issue**: Need to clearly define what counts as a "high-risk intervention" and thus requires repeat vitals, so the LLM judge knows when to expect a follow-up. Also should note exceptions, like if narrative explicitly states "no reassessment done because patient fought it" or something - then it's not an extraction error but an actual documented omission (which might be acceptable if explained).  
**Suggested Addition**: List the high-risk interventions explicitly in the prompt: _"High-risk interventions include: nitroglycerin for chest pain, opioid analgesics, benzodiazepines or antipsychotics for sedation, paralytics (e.g., succinylcholine, rocuronium) for RSI, intubation (airway placement), cardioversion or defibrillation."_[\[48\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT4.yaml#L9-L17) If any of these occur, expect a repeat vital sign or pertinent reassessment (pain score, blood pressure, mental status, etc.) afterwards in the output. Then add: _"Do not flag if the narrative clearly notes that no reassessment was done for some reason (e.g., 'unable to obtain BP after due to patient combativeness'). In such cases, the omission is in the source, not a Medic Copilot error."_ This aligns with inclusion criteria focusing on ground truth marked gaps[\[49\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT4.yaml#L28-L36). The working notes mention defining high-risk interventions and allowing documented omission reasons[\[159\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L54-L58) - those changes should be implemented.

By refining these prompts, the LLM judge will more reliably focus on true errors in semantic preservation and ignore clinically irrelevant differences. This reduces both false alarms and missed issues, making the evaluation more trustworthy. All prompt changes should be reviewed by a medical SME to ensure they cover the needed nuance without introducing bias.

## Denver Protocol Cross-Reference

To ensure the evaluation criteria cover documentation requirements from Denver Metro EMS protocols (July 2025), we cross-referenced key sections with current benchmarks:

| Protocol Section (2025) | Documentation Requirement (key points) | Current Benchmark Coverage | Gap? |
| --- | --- | --- | --- |
| **0050 Field Pronouncement**[\[160\]](https://www.dmemsmd.org/sites/default/files/DMEMSMD%20Protocols%20July%202025%20FINAL%202025-07-14.pdf#:~:text=%E2%80%A2%200050%20Field%20Pronouncement%20%E2%80%A2,%E2%80%A2%200070%20Lift%20Assist%2FMedical%20Assist)[\[30\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP5.yaml#L28-L36) | Document time of pronouncement, method (standing order vs base contact), physician name, and agency (e.g., coroner or medical direction). Preserve signs of obvious death if noted. | **A-CMP5** checks all four pronouncement fields[\[30\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP5.yaml#L28-L36). A-CAR1/A-CMP5 together ensure time and outcome documented. Obvious death signs, if mentioned (e.g., rigor mortis), would be captured under extraction (no specific benchmark, but unlikely to be missed). | **No gap** - A-CMP5 is aligned with 0050 requirements (all fields must be present) and is now a hard requirement[\[32\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP5.yaml#L20-L25). |
| **0051 Termination of Resuscitation**[\[93\]](https://www.dmemsmd.org/sites/default/files/DMEMSMD%20Protocols%20July%202025%20FINAL%202025-07-14.pdf#:~:text=) | Document criteria met for TOR, base contact for TOR, time of cessation, and that patient is a coroner's case if pronounced. (Essentially overlap with above when a code is stopped in field.) | **A-CAR1** ensures ROSC vs termination outcome is documented[\[83\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CAR1.yaml#L26-L34). **A-CMP5** would cover the pronouncement details for termination. Base contact for TOR: if narrative says base physician ordered stop, that should appear (we recommend ensuring base contact documentation). | _Minor gap_: If base contact physician is involved in TOR, need to document (as per 0051 E) - likely narrative would include it. We have recommended A-REF3 for base contact which can apply here too. Otherwise covered. |
| **0032 Patient Refusal of Transport**[\[161\]](https://www.dmemsmd.org/sites/default/files/DMEMSMD%20Protocols%20July%202025%20FINAL%202025-07-14.pdf#:~:text=Documentation%20Requirements%20for%20Refusal%20%E2%80%A2,care%20unless%20standing%20order%20refusal) | Confirm decision-making capacity, EMS interventions offered and declined, risks explained, patient understands risks, may re-engage EMS if needed, base physician name if required, and a signed refusal (or witness) form. | **A-REF1** covers risk explanation and patient understanding[\[98\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/rubrics/A-REF.yaml#L18-L26). **A-REF2** covers signed refusal/witness[\[98\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/rubrics/A-REF.yaml#L18-L26). **A-BHV2** covers capacity documentation (if capacity in question)[\[162\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/rubrics/A-BHV.yaml#L18-L26)[\[138\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-BHV2.yaml#L28-L36). These align with confirming capacity and risk discussion. Base physician name - not explicitly in benchmarks, we suggest adding that (gap). Reminding patient they can call back is often narrative text; not critical to evaluate extraction of that specific line (would likely be present if said). | **Yes, slight gap** - absence of explicit check for documenting base contact physician on required refusals. We plan to address by expanding A-REF prompts or adding A-REF3. Otherwise, main elements are covered (risks: A-REF1, signature: A-REF2, capacity: A-BHV2). |
| **0050/0051 Cardiac Arrest (Medical)** | Document initial rhythm, interventions (shocks, drugs with doses, airway), patient response (ROSC or not), times of key events, and if terminated: time and physician. If TOR criteria met, document that no ROSC and time called. | **A-CAR2** covers initial rhythm, shocks, drugs, airway details[\[90\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CAR2.yaml#L28-L36). **A-CAR1** covers ROSC or termination outcome with time[\[83\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CAR1.yaml#L26-L34). Times of interventions are not explicitly evaluated but likely captured if narrated (not a separate benchmark). Physician for TOR same as base contact gap above. Overall, the eval ensures the crucial data from a code are preserved. | **No major gap** - The content is covered. Only note is again documenting which physician gave the order if applicable - low frequency scenario but to be safe, included in base contact suggestion. |
| **3000 Acute Coronary Syndromes/STEMI** | Document 12-lead ECG results/interpretation, aspirin administration (or contraindication), nitroglycerin (if given or why not), IV access, pain relief if given, and that a STEMI alert was called to hospital (if STEMI identified). | **A-PRT2** specifically targets aspirin given, 12-lead done, nitro, and STEMI alert documentation[\[163\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT2.yaml#L10-L18). It flags missing pieces in those steps[\[164\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT2.yaml#L36-L39). **A-SFT2** would cover if aspirin not given due to allergy, as a contraindication note. Pain relief medication would be captured by extraction (not specifically checked, but less critical than ASA/Nitro for protocol compliance). | **No gap** - A-PRT2 aligns perfectly with protocol, and covers calling the STEMI alert[\[163\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT2.yaml#L10-L18). The needed elements for STEMI care are tested. |
| **4000 Stroke (e.g., 4030)** | Document stroke scale findings (Cincinnati/CPSS or RACE etc.), "Last Known Well" time, glucose check (to rule out mimics), any stroke alert activation, and neurological exam details. | **A-STR1** covers stroke scale and neuro exam findings documentation[\[106\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-STR1.yaml#L28-L36). **A-STR2** covers Last Known Well time and stroke alert notification[\[111\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-STR2.yaml#L28-L36). Glucose check - not explicitly in a benchmark, but if narrative said "BGL 120 mg/dL" and output missed it, that would be caught by A-FCT1 (vitals) or completeness. It's lower criticality than LKW and exam. | **No gap** - stroke scale and LKW/alert are the big ones and are covered by A-STR benchmarks. Glucose is minor and already part of vitals extraction potentially. |
| **6000 Behavioral (General)** | Document patient's decision-making capacity, any use of restraints (physical or chemical), rationale for restraints, and patient response. If applicable, document initiation of an M1 hold (72-hr psych hold) or law enforcement custody. If patient refuses but lacks capacity, document implied consent. | **A-BHV1** covers documenting restraints and chemical sedation used[\[134\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-BHV1.yaml#L8-L16)[\[132\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-BHV1.yaml#L28-L36). **A-BHV2** covers capacity assessment and any hold/consent status[\[138\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-BHV2.yaml#L28-L36)[\[139\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-BHV2.yaml#L39-L42). These directly address the key points (rationale for restraint is usually implicit - the narrative states combative behavior, which would be captured in narrative text; output just needs to show restraint was applied). M1 hold or police custody is covered under capacity/consent (the prompt mentions 5150 hold etc.)[\[138\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-BHV2.yaml#L28-L36). Implied consent in an incapacitated patient also falls under A-BHV2's scope. | **No gap** - The benchmarks hit the major documentation needs: if they tied someone down or sedated them, it's documented; if patient was not competent, that's documented. Rationale (like what behavior) is likely present as narrative context anyway. |
| **1130 Patient Restraint Procedure** (Denver-specific) | Document indications for restraint, method (type of restraint), time applied, monitoring of patient condition every 5-15 min, and results. Also document any injuries from restraint. | **A-BHV1** ensures the act of restraint/sedation is documented[\[132\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-BHV1.yaml#L28-L36). It doesn't specifically check that "every 15 min vitals" were charted - however, if the narrative had repeat vitals it should be in output anyway (and A-SFT4 would catch if they omitted a required reassessment). Injuries from restraint or struggle would be just part of narrative findings (no specific test, but would likely be captured by extraction if mentioned). The eval doesn't explicitly enforce the q15 monitoring documentation (that might be too granular). But at least one post-restraint check should be present - which A-SFT4 would cover if sedatives given. | _Minor gap_: Continuous monitoring documentation isn't explicitly tested. Given complexity, probably acceptable. If needed, we rely on general completeness and A-SFT4 for major omissions. The critical part (that restraints were used at all) is covered. |
| **8000 Trauma (General)** | Document mechanism of injury, GCS (initial and any changes), vital signs, pertinent exam findings (e.g., distal neuro status for extremity injuries), interventions (tourniquets, etc.), and trauma alert if criteria met. | **A-TRM1** covers GCS/neuro status documentation[\[115\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TRM1.yaml#L28-L36). **A-TRM2** covers critical interventions (tourniquet, needle decompression, etc.)[\[119\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TRM2.yaml#L28-L36). Vital signs are covered by A-FCT1 (for extraction) and trends by A-TMP2. Mechanism of injury is expected in narrative/dispatch; not explicitly a benchmark but would fall under completeness if entire dispatch/arrival section were missing (A-CMP1/3). Trauma alert - we noted this is not currently a benchmark (see gap). | **Yes, slight gap** - "Trauma Alert" notification is not explicitly tested. If medics call one, it should be in output akin to stroke/STEMI alerts. We proposed A-TRM3 to cover this. Mechanism detail omission could be a gap, but likely low impact as narrative usually carries it. |
| **1000 Airway - RSI Intubation** | Document indication for intubation, medications used (sedative & paralytic with doses), number of attempts, ETT size and position, confirmation methods (ETCO₂ value, breath sounds, etc.), and any post-intubation sedation. | **A-PRT1** covers nearly all of these: it lists indication, preoxygenation, sedative/paralytic and their doses, ETT size/depth, EtCO₂ confirmation, and post-intubation sedation if given[\[53\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT1.yaml#L10-L18)[\[54\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT1.yaml#L29-L37). Number of attempts is not explicitly listed, which we flagged (if mentioned in narrative, currently output could omit without failing A-PRT1, since it's not a checked item). That's a minor documentation point, though often recorded. All other elements are covered. | **Minor gap** - Intubation attempts count. If narrative says "2 attempts," output should note it. Could incorporate that into A-PRT1's criteria or leave it; given the comprehensive nature of A-PRT1, this is small. Otherwise, A-PRT1 aligns with protocol 1000 expectations well. |
| **7000 Obstetrics (e.g., Imminent Birth)** | Document gravid/para, complications (like preeclampsia, gestational age), time of birth, newborn assessments (APGAR scores, gender, any interventions), and if multiple births. Also note if mother had complications (hemorrhage, etc.). | **A-OBS2** covers gravida/para, gestational age, and pregnancy complications[\[129\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-OBS2.yaml#L28-L36). **A-OBS1** covers newborn status (APGAR, neonate condition, any resuscitation)[\[124\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-OBS1.yaml#L28-L36). Time of birth isn't explicitly a benchmark but presumably would be documented as an event. Multiple birth scenario not explicitly tested, but A-OBS1 would likely fail if two babies were born and output only mentions one. Placental delivery or maternal hemorrhage would be narrative items - not specifically tested, but any omission might slightly hit completeness. | **No significant gap** - The major OB documentation points are covered. If twin deliveries become part of test cases, we might need to ensure output captures both - possibly add scenario-based checks. Generally, current benchmarks reflect protocol needs (APGAR, etc.). |
| **7010 Neonatal Care** | Document interventions on newborn (warming, stimulation, oxygen, CPR if done), APGARs, and status changes. | **A-OBS1** includes neonatal resuscitation steps if performed[\[125\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-OBS1.yaml#L9-L12)[\[126\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/3504cf46ccaae80b4d05f379137945e3a01f7257/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-OBS1.yaml#L38-L41) ("stimulated, dried, O₂ given" in example). It will flag if output says nothing beyond "delivery". So that covers newborn care steps. CPR on a newborn would be an intervention that should be noted (rare, but would be caught by the same principle). | **No gap** - A-OBS1 seems to handle it. It expects neonatal interventions to be documented, aligning with neonatal protocol which emphasizes documenting those steps. |

In summary, the evaluation framework is largely in lockstep with the Denver documentation standards. Only a couple of minor gaps were noted (base contact documentation and trauma alert), which we've addressed with recommendations. Everything else - from refusals to intubations to stroke - is well represented in the benchmarks.

## Priority Action Items (for medical SME)

- **\[High\] Finalize Threshold Updates for Safety-Critical Benchmarks** - Implement the threshold changes to 1.0 for A-SFT1, A-SFT2, A-FMT1, A-CMP5 (as already listed in WORKING.md)[\[165\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L13-L22), and set A-REF1 and A-BHV1 to hard 1.0 as well. These ensure non-negotiable documentation elements (allergies conflicts, unsafe doses, refusal risks, restraints, etc.) must always be correct in output.
- **\[High\] Refine LLM Judge Prompts with Clinical Nuance** - Apply the prompt modifications: add negation phrase synonyms and the "unknown vs denied" rule[\[3\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L34-L41); clarify double negatives and "but" in negation_complex[\[153\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L38-L41); update contradiction to ignore temporal changes and include a "AOx4 but confused" example[\[20\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L42-L46); adjust vitals prompt to allow unit conversions/rounding[\[156\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L46-L50); add diagnosis synonym guidance to impression_mismatch[\[158\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L50-L54); and enumerate high-risk interventions + exceptions in repeat_vitals prompt[\[159\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/47b5e1c6cc700150d2c9e6dce51ce88ddc06c3b0/zy_experimental/soar-pydantic-eval/WORKING.md#L54-L58). These changes will reduce evaluation errors and align the LLM judging with clinical expectations.
- **\[Medium\] Introduce Base Contact Documentation Check** - Create a new benchmark or expand existing ones (Refusal or System Logic category) to verify that when medics consult a base physician (for refusals, TOR, medication orders outside protocol), Medic Copilot notes it. This includes the physician's name and any outcome (e.g., physician agreed with plan). This addition will close the loop on scenarios requiring online medical direction, which are currently not explicitly tested and are important for legal documentation[\[103\]](https://www.dmemsmd.org/sites/default/files/DMEMSMD%20Protocols%20July%202025%20FINAL%202025-07-14.pdf#:~:text=%E2%80%A2%20Patient%20reminded%20they%20may,candidate%20for%20an%20alternative%20disposition).
- **\[Medium\] Allergy/History Extraction Completeness** - Add a low-weight benchmark to ensure stated allergies and major history items in the narrative appear in the output's patient history sections (when they don't directly tie into an intervention). This will improve completeness for future use of records (preventing a scenario where an allergy isn't documented simply because it didn't come into play during that call).
- **\[Low\] Trauma Alert Documentation** - If feasible, incorporate a test scenario for trauma team activation and ensure Medic Copilot documents it similar to STEMI/Stroke alerts. Not a common omission, but for consistency in high-acuity cases it's worth verifying.
- **\[Low\] Additional Edge Case Testing** - Consider adding a twin birth scenario to **A-OBS1** to ensure multiple newborns are each documented. Similarly, a burn patient scenario for **A-TRM/Extraction** to test capturing burn size/degree details. These are lower priority but would further strengthen the eval suite's coverage of atypical cases.

By addressing these action items, we will tighten the evaluation harness to ensure no clinically significant detail is overlooked by Medic Copilot. Each of these recommendations focuses on the **fidelity of documentation** - reinforcing that Medic Copilot must capture exactly what was said and done, which ultimately supports better patient care continuity, billing accuracy, and medicolegal soundness.