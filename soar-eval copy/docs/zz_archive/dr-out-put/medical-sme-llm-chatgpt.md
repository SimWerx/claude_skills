# Medical SME Review: SOAR Benchmark Validation

## Executive Summary

Overall, the existing SOAR benchmarks capture many key clinical documentation quality areas for Medic Copilot, but a few critical gaps remain. The current rubrics and benchmarks are largely sound, covering negation, fact extraction, safety, protocol adherence, completeness, temporal order, evidence linkage, and format. However, important scenario-specific errors (pediatric dosing, cardiac arrest details, stroke scales, trauma specifics, OB/neonatal data, behavioral/restraint documentation, and refusal-of-care elements) are not yet addressed. Priority should be given to **pediatric weight-based dosing safety**, **cardiac arrest event documentation**, and **refusal documentation**, as omissions in these areas could directly impact patient safety and medicolegal outcomes.

## Priority Action Items (for paired programmer)

- **\[High\] Implement New Benchmarks** - Add the proposed rubrics and benchmarks (A-PED, A-CAR, A-STR, A-TRM, A-OBS, A-BHV, A-REF) to cover the identified gaps. Start with **Pediatric Dose Check (A-PED1)** and **Cardiac Arrest Outcome (A-CAR1)** as highest priority due to patient safety implications. Define YAML specs for each (following existing patterns) and integrate any regex or code checks needed (e.g., weight-based dose logic, presence of "ROSC" in narrative). Ensure these benchmarks tie into appropriate rubric structure (likely each as its own rubric for modularity).
- **\[High\] Adjust Critical Thresholds** - Update the YAML for A-SFT1, A-SFT2, A-FMT1 (and A-CMP5, A-EVD1 if decided) to the recommended thresholds. For A-SFT\*, also adjust the rubric threshold if needed (A-SFT rubric passing_threshold could be set to 1.0 to reflect any failure = rubric fail)[\[19\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/rubrics/A-SFT.yaml#L11-L19). After changing, run tests on known cases to ensure the scoring logic still behaves (with MIN, it should). The goal is to enforce no tolerance on those critical errors.
- **\[High\] Revise LLM Judge Prompts** - Edit the prompt markdown files as per suggestions: include additional negation phrases in _negation_simple_prompt.md_, clarify complex negation with "but/except" in _negation_complex_prompt.md_, highlight simultaneous contradiction vs temporal change in _contradiction_prompt.md_, add unit conversion tolerance in _vitals_extraction_prompt.md_, refine equivalence vs mismatch in _impression_mismatch_prompt.md_, and define high-risk interventions in _repeat_vitals_prompt.md_. After editing, re-run a few evaluation examples through the LLM to verify it now handles the edge cases (e.g., a test where the only difference is Fahrenheit vs Celsius should now return false for misinterpretation).
- **\[Medium\] Update Documentation & Guides** - Reflect these changes in the project docs: e.g., add the new rubrics to the rubrics/CLAUDE.md table and describe their fields, update docs/corpus-validator-mapping.md if any new validators map from corpus (like pediatric dosing might tie into known weight check logic). Also add a note in WORKING.md about these new benchmarks and thresholds so team is aware of the stricter requirements.
- **\[Medium\] Test on Real/Simulated Cases** - Using sample transcripts (especially ones involving kids, arrests, strokes, etc.), run Medic Copilot and the new evaluation to see if any false positives/negatives arise. Fine-tune regex patterns or LLM prompt wording based on these pilot tests. For instance, test a refusal scenario to ensure A-REF benchmarks properly catch missing risk documentation, etc., and adjust accordingly.
- **\[Medium\] Monitor Impact on Score Aggregation** - With more benchmarks and higher thresholds, some traces that passed before might now fail. Identify if any existing test corpus cases fail under the new rules unexpectedly. If so, verify whether it's a true error (then the stricter rule is correct) or an edge case requiring an adjustment. Pay special attention to A-EVD1 going to 0.90 - ensure trivial unmatched bits aren't causing failures (adjust inclusion criteria if needed).
- **\[Low\] Consider Weight Rebalance of Rubrics** - After implementing everything, re-evaluate if rubric weights should be tuned (e.g., Safety might remain 0.125 because we handle criticality via threshold/MIN, but if we wanted to emphasize new rubrics like A-PED or A-CAR, maybe they get some weight from others). This is low priority initially - functional correctness comes first - but keep in mind for final scoring calibration.
- **\[Low\] Future: Automated Image/ECG checks** - (Not immediately actionable, but forward-looking) In STEMI or stroke, sometimes medics transmit ECG images or CT images. Medic Copilot doesn't handle images now, but if it did, evidence attribution might need expansion. Just an item to note in backlog; no current action needed beyond textual.

By executing these steps, we will address the major clinical concerns and make the Medic Copilot evaluation more robust and aligned with real-world EMS documentation standards

## Benchmark-by-Benchmark Clinical Review

### A-NEG: Negation Handling

**Clinical Assessment**: Needs Revision  
**Findings**: - The negation handling benchmarks cover _simple negation_ ("denies X," "no Y"), _double negatives/complex phrasing_ ("not denying X," "no other complaints"), _implicit or hedged negation_ ("unsure if X," "possibly denies"), and _internal contradictions_, which is comprehensive[\[1\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/rubrics/A-NEG.yaml#L16-L24)[\[2\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-NEG3.yaml#L27-L35). This aligns with common EMS narrative patterns where medics list negatives (e.g., "denies chest pain or SOB"). - One EMS-specific pattern not explicitly mentioned is the use of conjunctions like **"but"** to indicate exceptions or contradictions. For example, **"Patient AOx4 _but_ confused"** presents a nuance where two statements conflict in one sentence. Currently, contradiction detection (A-NEG4) looks for conflicting findings across sections and times[\[3\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-NEG4.yaml#L28-L36), but may miss single-sentence contrasts if "but" is not flagged. Similarly, phrases like **"no evidence of…"** or **"without \[symptom\]"** function as negations and should be interpreted as such, but it's unclear if the LLM prompts cover those synonyms. - The LLM prompt for simple negation focuses on "denies/no" examples[\[4\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/evaluators/llm-judge/negation_simple_prompt.md#L4-L12) and the complex negation prompt likely covers "not denying" and "no other" constructions. However, **"except"** or **"but"** phrases (e.g., "no complaints except mild dizziness") could be missed edge cases not explicitly listed. These could lead to the AI misinterpreting a remaining complaint as absent. **Recommended Changes**: - \[ \] **Include "but/however" contradiction cues**: Augment negation/contradiction logic to catch phrasing like "X but Y" where Y negates or contradicts X. For example, "AOx4 but confused" should trigger a flag under A-NEG4 for internal inconsistency. The LLM prompt or code for contradictions should treat conjunctions like "but" or "however" as potential split points for conflicting clauses. - \[ \] **Expand negation phrase list**: Ensure that common EMS negation synonyms are covered. Add to A-NEG1/A-NEG2 detection patterns for phrases such as "without **&lt;symptom&gt;**," "negative for **&lt;finding&gt;**," and "no evidence of **&lt;finding&gt;**," since these are functionally equivalent to "no **&lt;finding&gt;**." This will improve polarity accuracy, as EMS reports often use "negative for" or "without" in physical exams. - \[ \] **Refine LLM prompts for nuance**: Update the negation_simple_prompt.md and negation_complex_prompt.md to instruct the model to ignore clinically irrelevant wording differences and focus only on true polarity errors. For example, explicitly note that phrases like "denies any other issues **except** X" should be interpreted as X being affirmed, not negated. This addition will help the LLM judge catch tricky cases where a negation has an exception clause.

### A-FCT: Fact Extraction

**Clinical Assessment**: Needs Revision  
**Findings**: - The Fact Extraction rubric addresses vital signs, medications, procedures, and impressions[\[5\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/rubrics/A-FCT.yaml#L16-L24). A-FCT1 (Vitals Extraction) carries a heavy weight (50% of rubric) and high threshold (0.90)[\[6\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FCT1.yaml#L10-L19)[\[7\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FCT1.yaml#L22-L30), reflecting that extracting vitals correctly is crucial. This is appropriate - mis-documenting a BP or heart rate can mislead care. The inclusion of medication and dose matching in A-FCT1 (as indicated by reuse of med extraction patterns[\[8\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/docs/corpus-validator-mapping.md#L92-L96)) means Medic Copilot is expected to capture administered meds accurately. However, **pediatric vs. adult distinctions** in extraction need attention: - _Vital Signs:_ The evaluation currently checks if values match exactly, allowing minor rounding[\[9\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FCT1.yaml#L32-L40). This is fine across age groups for factual accuracy. There is no direct check on whether a value is physiologically plausible for the patient's age - but since Medic Copilot is retrospective, we only demand matching the narrative, not interpreting it. Still, in pediatric cases, vitals might be recorded differently (e.g., systolic BP norms, heart rate ranges). The system should at least be tested on extremes (e.g., infant heart rate 180 marked as 180, not "80" or some misinterpretation). No explicit pediatric vital ranges are referenced in the benchmark, so this might be acceptable as-is, focusing purely on fidelity to the source. - _Medications:_ The rubric emphasis on dose accuracy[\[5\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/rubrics/A-FCT.yaml#L16-L24)[\[10\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/rubrics/A-FCT.yaml#L25-L28) is clinically appropriate - EMS medications must be recorded exactly as given. One gap is **pediatric dosing units/expressions**. Paramedics often document pediatric doses in mg/kg or reference Broselow tape colors for equipment/drug sizes. If the narrative says "given 0.1 mg/kg Epi" or "Broselow orange zone," Medic Copilot must translate that into a total dose or appropriate fields. There isn't a specific benchmark for pediatric dose calculation or conversion errors. Given that **all pediatric medications are weight-based**[\[11\]](https://www.chawisconsin.org/wisconsin-emergency-medical-services-for-children-weighs-in-on-improving-pediatric-care-free-broselow-compatible-weight-measuring-tapes-for-all-emergency-medical-service-agencies/#:~:text=administration%2C%20as%20all%20pediatric%20medications,the%20highly%20charged%20clinical%20situations), a failure to properly interpret a weight-based dose (e.g., treating "0.1 mg/kg" as a flat 0.1 mg without multiplying by weight) would be a serious factual error that is not explicitly tested. - _Demographics and other facts:_ Age, weight, and GCS are examples of key facts that should be extracted. These fall under general "clinical facts," but no current benchmark explicitly verifies age or weight extraction accuracy. Yet, **patient weight is critical for pediatric cases** - omission of weight could cascade into dose errors. - Example: If a scenario involves a 4-year-old ~16 kg child and the medic narrates "Given 0.3 mg of Epi (0.01 mg/kg) after estimating weight via Broselow," the system needs to capture the 0.3 mg Epi dose. If it missed that or recorded it incorrectly, currently A-FCT1 might catch it (dose mismatch) but there's no special mention of using the weight properly. The benchmarks don't mention Broselow or pediatric tape usage at all. **Recommended Changes**: - \[ \] **Add pediatric extraction considerations**: Include guidance or sub-checks in A-FCT1 for pediatric scenarios. For example, if a weight or age is present in the input, ensure it's reflected in output (perhaps as a documented weight field or pediatric age group field). While not all schemas include patient weight, Medic Copilot should at least not ignore it if given. This ties into safety: accurate weight documentation is "paramount to safety when treating children"[\[12\]](https://www.chawisconsin.org/wisconsin-emergency-medical-services-for-children-weighs-in-on-improving-pediatric-care-free-broselow-compatible-weight-measuring-tapes-for-all-emergency-medical-service-agencies/#:~:text=%E2%80%9CMedication%20dosing%20accuracy%20is%20paramount,%E2%80%9D). A specific new benchmark (see A-PED proposal below) is suggested for pediatric dose errors, but A-FCT1/A-FCT4 can flag obvious extraction misses (like entirely missing a medication that was given). - \[ \] **Vital units and rounding rule in prompt**: The LLM prompt for vitals extraction (vitals_extraction_prompt.md) should explicitly allow equivalent units. The rubric says 98.6°F vs 37°C is acceptable[\[13\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/rubrics/A-FCT.yaml#L20-L24), but the prompt text should mention that a correctly converted value is _not_ an error. If not already, add a line like "Ignore differences that are just unit conversion (Fahrenheit/Celsius) or trivial rounding." This ensures the LLM doesn't erroneously penalize clinically irrelevant discrepancies. - \[ \] **Ensure impression matching is clinically framed**: A-FCT4 checks final impression alignment[\[14\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FCT4.yaml#L10-L19). The inclusion/exclusion looks good (only flag if the chosen diagnosis is outright wrong given ground truth, not if it's a reasonable alternative[\[15\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FCT4.yaml#L28-L36)). We should confirm the LLM prompt (impression_mismatch_prompt.md) accounts for synonyms (e.g., "MI" vs "myocardial infarction" vs "STEMI"). It likely does (the rubric allows minor wording differences[\[16\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FCT4.yaml#L20-L25)), but adding explicit examples of synonyms in the prompt could help. For instance, instruct the LLM that "acute myocardial infarction" vs "STEMI" are equivalent impressions so it doesn't count that as a mismatch. This will reduce false flags where the AI uses slightly different terminology that is clinically synonymous.

### A-SFT: Safety Flags

**Clinical Assessment**: Needs Revision  
**Findings**: - The Safety Flags rubric appropriately targets high-risk errors: **unsafe medication dosing or routes (A-SFT1)**, **allergy/contraindication oversights (A-SFT2)**, and **missing post-intervention reassessment (A-SFT4)**[\[17\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/rubrics/A-SFT.yaml#L16-L24)[\[18\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/rubrics/A-SFT.yaml#L34-L40). These reflect real EMS patient safety issues. The rubric uses **MINIMUM aggregation** and a high passing threshold (90%)[\[19\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/rubrics/A-SFT.yaml#L11-L19)[\[20\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/rubrics/A-SFT.yaml#L26-L33), meaning any single serious safety miss fails the rubric - a design that aligns with the concept of a "never event." This is very appropriate: even one occurrence of, say, a gross overdose or giving a drug despite a known severe allergy, is unacceptable. - **A-SFT1 (Dose/Route safety)**: Clinically, this benchmark is on point. It looks for doses outside safe ranges (taking into account adult vs pediatric) or wrong routes (like IM nitroglycerin)[\[21\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT1.yaml#L10-L19)[\[22\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT1.yaml#L28-L36). The examples given (aspirin 3240 mg instead of 324 mg, nitro 4 mg IM instead of 0.4 mg SL) are classic critical errors[\[23\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT1.yaml#L38-L41). The inclusion explicitly says to consider patient type (adult/pediatric)[\[24\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT1.yaml#L26-L34), which is excellent - e.g., giving an adult dose to a child should be caught. One gap is that _underdoses_ are not flagged (since those are less immediately dangerous), which is reasonable for "safety" focus. However, extremely low dosing could be a care quality issue (but that might fall under protocols or completeness rather than safety). - A possible omission: **weight-based dosing errors** beyond just "too high for age." For example, if Medic Copilot recorded the _wrong calculation_ of a peds dose (not merely high or low, but arithmetically incorrect relative to a stated mg/kg), would A-SFT1 catch it? It might, as an "obviously incorrect" dose. But maybe not if the dose seems plausible but is wrong given weight. This nuance might require a pediatric-specific check. - Another omission: **Medication interaction contraindications** (e.g., nitroglycerin given to a patient on phosphodiesterase inhibitors like Viagra). The benchmark focuses on allergies/medical history contraindications (A-SFT2) and dose, but not drug-drug interactions. EMS protocols often warn against certain combos (the Minnesota STEMI guideline, for instance, says to hold nitrates if recent ED drugs were taken[\[25\]](https://www.heart.org/-/media/Files/Affiliates/MWA/MN-ML-EMS-Transport-Guideline.pdf#:~:text=%EF%82%B7%20Administer%20Chewable%20Aspirin%2081,90)). Currently, if the narrative noted "patient on Viagra" and Medic Copilot still listed nitro given without comment, **no benchmark explicitly flags that**. It's a corner case but clinically important. - **Vital sign contraindications**: Similarly, giving medications despite vital sign contraindications (e.g., nitro with systolic BP <90, as protocols forbid[\[26\]](https://www.heart.org/-/media/Files/Affiliates/MWA/MN-ML-EMS-Transport-Guideline.pdf#:~:text=Tadalafil%20%28Cialis%2C%20Adcirca%29,90)) is not directly checked. If Medic Copilot doesn't flag it, that could be dangerous. However, this is tricky because it requires interpreting vitals relative to actions, which may be beyond current system scope. Still, it's worth noting as a gap - the AI might document "Nitroglycerin given" even if BP was 80, and no alert would be raised by current benchmarks. - **A-SFT2 (Contraindications)**: This addresses allergy or documented history conflicts[\[27\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT2.yaml#L28-L36). It is well-founded. The examples (penicillin given despite penicillin allergy, aspirin given despite aspirin allergy) are clear "red lines"[\[28\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT2.yaml#L38-L44). This benchmark has threshold 0.95, essentially requiring perfection aside from maybe one minor miss[\[29\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT2.yaml#L14-L22). That is justified - these are classic never events (you _must_ catch an allergy conflict 100% of the time). One extension: **other contraindications** in history beyond allergies, e.g., giving a medication contraindicated in pregnancy or in a patient with a specific condition (like giving glucagon to a pheochromocytoma patient, or a beta-blocker to a severe asthma patient if documented). The benchmark text does mention "medical history" and specific conditions (QT prolongation risk, etc.)[\[27\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT2.yaml#L28-L36), so it's fairly comprehensive. - **A-SFT3 (Route safety)**: The rubric YAML listed a separate A-SFT3 for route errors[\[18\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/rubrics/A-SFT.yaml#L34-L40), but the implementation seems to have merged route issues into A-SFT1's concept (as evidenced by label "Dose or Route Unsafe"[\[30\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT1.yaml#L10-L17)). This is acceptable, because wrong route (e.g., IV vs IO vs IM) typically would manifest as either an obviously wrong choice (like IM for a drug that should be IV) or simply be captured as part of "unsafe administration." Combining them means any route error triggers failing A-SFT1, which is reasonable - giving a med via the wrong route can be as dangerous as wrong dose. - **A-SFT4 (Reassessment after high-risk intervention)**: This is an important patient safety aspect often overlooked in documentation. Paramedics are expected to re-check vitals, pain, or key indicators after interventions like NTG, fentanyl, RSI, etc. The benchmark correctly flags if Medic Copilot's output fails to include a repeat assessment after such events[\[31\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT4.yaml#L10-L19)[\[32\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT4.yaml#L27-L36). For example, if nitroglycerin was given, one would expect a follow-up blood pressure and pain score; if an RSI was done, one expects a post-intubation EtCO₂ reading and maybe a sedation level. The benchmark's inclusion criteria and examples align with these expectations[\[33\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT4.yaml#L28-L37). One challenge: how does the system know an intervention was "high-risk" and needed reassessment? Likely via a list of trigger meds (the prompt or code might list medications like NTG, morphine/fentanyl, intubation). The current LLM prompt (repeat_vitals_prompt.md) should list those triggers. If it doesn't enumerate them, the LLM might miss some ("e.g., after any **significant intervention** like medication administration or procedure, check for follow-up vitals"). It might be safer to explicitly mention common high-risk interventions in the prompt (NTG, opioids, benzodiazepines, paralytics, cardioversion, etc.). - The weighting of each safety sub-benchmark at 0.25 each (or effectively equal, since any fail is critical) is fine. In practice, A-SFT1 and A-SFT2 are absolute musts (hence 95% thresholds), whereas A-SFT4 has a 90% threshold[\[34\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT4.yaml#L13-L21) - implying a small allowance that occasionally a follow-up might not be documented if truly not done. This is clinically tolerable; sometimes vitals aren't repeated due to evacuation or sudden change of plans, but generally they should be. **Recommended Changes**: - \[ \] **Make dose/allergy truly never-events**: Consider raising **A-SFT1 and A-SFT2 thresholds to 1.0** (and rubric threshold to 1.0) to indicate zero tolerance. Currently they're 0.95[\[35\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT1.yaml#L14-L22)[\[29\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT2.yaml#L14-L22), but it's hard to imagine any "acceptable" level of these errors. With MIN aggregation, functionally 0.95 vs 1.0 might not change much (since any significant error will drop the score to 0), but setting 1.0 makes the standard explicit. These are analogous to wrong-site surgery in documentation - **no** occurrence is acceptable. - \[ \] **Incorporate vital sign contraindication logic**: If feasible, extend A-SFT2 or A-SFT1 checks to catch obvious protocol violations related to vitals. For example, Medic Copilot should flag if nitroglycerin is documented when SBP < 90 (unless a rationale is given). This could be a simple code check: if output shows nitro given and a blood pressure field exists <90, that's a fail. This may require Medic Copilot to record pre-medication vitals (which it should in timeline), and the evaluator can cross-check. It adds complexity but addresses a **common EMS safety issue** (nitroglycerin-induced hypotension). Indeed, EMS protocols mandate holding nitrates for hypotension or recent ED meds[\[26\]](https://www.heart.org/-/media/Files/Affiliates/MWA/MN-ML-EMS-Transport-Guideline.pdf#:~:text=Tadalafil%20%28Cialis%2C%20Adcirca%29,90). Automating this check would improve patient safety flagging. - \[ \] **Expand contraindication list**: Ensure A-SFT2's rules include _drug-drug interactions_ and other non-allergy contraindications. For instance, add logic for the PDE-5 inhibitor + nitro scenario (as per AHA guidelines, nitrates are contraindicated within 24-48h of sildenafil/vardenafil[\[25\]](https://www.heart.org/-/media/Files/Affiliates/MWA/MN-ML-EMS-Transport-Guideline.pdf#:~:text=%EF%82%B7%20Administer%20Chewable%20Aspirin%2081,90)). If the narrative mentions those meds or the patient's use of them (which might appear in history or meds), the absence of any warning or withholding in output should trigger a fail. This could be a new entry (e.g., A-SFT3 if reintroducing separate index, or just an added rule in code). - \[ \] **Refine repeat_vitals_prompt.md**: Make sure it clearly defines what counts as a "high-risk intervention" requiring reassessment. For example: _"Focus on whether any critical intervention (e.g., analgesic given, sedative given, intubation, vasoactive drug given) has an appropriate follow-up vital sign or status check."_ Currently, A-SFT4's concept covers nitro, RSI meds, fluid bolus, etc.[\[31\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT4.yaml#L10-L19). The prompt should instruct the LLM to look for those events. Also, include that if the narrative explicitly notes no reassessment was done for a reason, that might be acceptable (though usually one would still flag it as a documentation gap). Clarifying this will help capture edge cases. - \[ \] **Document rationale for overrides**: Encourage that if Medic Copilot ever documents a known contraindicated action, it should include a rationale (e.g., "Aspiring given despite allergy, crew unaware of allergy at that time" or "No alternative, risk deemed acceptable"). While the AI might not fabricate rationale on its own, if it did include one from medic's notes, the evaluator could then consider not failing it. The evaluation criteria could note that a **documented exception** prevents auto-failure. (This aligns with rubric acceptable deviations: "override with documented rationale"[\[36\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/rubrics/A-SFT.yaml#L20-L27).) Coding this exception prevents penalizing cases where medics consciously decided to proceed and documented why.

### A-PRT: Protocol Tracking

**Clinical Assessment**: Needs Revision  
**Findings**: - The Protocol Tracking rubric focuses on **high-acuity care pathways** where certain steps must be documented for quality and medicolegal completeness[\[37\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/rubrics/A-PRT.yaml#L7-L15). It explicitly calls out **STEMI**, **RSI**, and **DOA (death in field)** scenarios[\[38\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/rubrics/A-PRT.yaml#L16-L24). This is very much in line with current guidelines: agencies and medical directors expect full documentation of critical interventions in those cases for QA and legal defense. The two benchmarks: - **A-PRT1 (RSI Critical Fields)** ensures that during a Rapid Sequence Intubation, the output includes all required elements: indication for intubation, confirmation that patient was pre-oxygenated, the drugs used (sedative and paralytic with doses), the ETT size and depth, and confirmation of placement (EtCO₂ and other methods)[\[39\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT1.yaml#L10-L18)[\[40\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT1.yaml#L28-L36). The weight is 0.50 (half the rubric) and threshold 0.90[\[41\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT1.yaml#L12-L20), indicating we allow maybe one minor field missing but generally require most items. This reflects how crucial these are - missing any could compromise care. The elements listed align with standard RSI protocol checklists (e.g., NAEMSP guidelines and many EMS protocols require documenting **sedation, paralysis, tube confirmation with ETCO₂**, etc.). For example, WV's state RSI protocol mandates confirming ETT placement with bilateral breath sounds and _quantitative end-tidal CO₂_[\[42\]](https://www.wvoems.org/media/456430/4903%20-%20rsi%20-%201-2021.pdf#:~:text=%28in%20about%2030%20,high%20intraocular%20pressure%2C%20high%20potassium), as well as recording all drugs used. A-PRT1's example where no paralytic dose or no EtCO₂ value is present[\[43\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT1.yaml#L38-L44)illustrates those exact gaps. One update: modern practice also expects **ongoing sedation/analgesia** post-intubation if the patient is paralyzed (often a maintenance sedative infusion or repeat doses to avoid awake paralysis). Medic Copilot should ideally capture if an ongoing sedation plan was noted or not. Currently, A-PRT1 doesn't explicitly mention post-intubation sedation or pain management. It might be a level of detail beyond the "critical fields" (which focus on initial RSI execution), but it is important for humane care. At least, if the narrative mentions starting a midazolam drip post RSI and the AI omitted it, that's a miss to consider. - **A-PRT2 (STEMI Care Protocol)** checks that in an Acute Coronary Syndrome/STEMI case, all standard care steps are present: e.g., **aspirin given**, **12-lead ECG performed and interpreted**, **STEMI alert activation to hospital** (cath lab notification), and other expected interventions[\[44\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT2.yaml#L10-L19)[\[45\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT2.yaml#L28-L36). Its threshold is 0.85 (some lenience)[\[46\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT2.yaml#L14-L22), which is understandable - perhaps not every STEMI will get every step (if contraindicated or already done by referring party). The benchmark example mentions missing aspirin and no STEMI alert as a failure[\[47\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT2.yaml#L38-L42). This aligns with AHA guidelines that _all_ STEMI patients should receive chewable aspirin promptly (162-325 mg) unless contraindicated[\[48\]](https://www.aafp.org/pubs/afp/issues/2009/0615/p1080.html#:~:text=ACC%2FAHA%20Guideline%20Update%20for%20the,recognition%20of%20symptoms%2C%20unless), and that EMS should activate the receiving center as early as possible with a STEMI alert[\[49\]](https://www.heart.org/-/media/Files/Affiliates/MWA/MN-ML-EMS-Transport-Guideline.pdf#:~:text=%EF%83%BC%20Document%20Date%20and%20Time,a%20STEMI%20receiving%20center%2C%20receive). Additionally, obtaining a prehospital 12-lead ECG for chest pain is an expectation nearly 100% of the time[\[50\]](https://www.heart.org/-/media/Files/Affiliates/MWA/MN-ML-EMS-Transport-Guideline.pdf#:~:text=1.%20All%20patients%20with%20non,30%20minutes). A-PRT2 seems to cover these: if any of those critical steps (ASA, ECG, alert) are missing in output, it flags it. One thing to ensure: if something was omitted due to a documented contraindication (e.g., no aspirin because of a true aspirin allergy), that should perhaps not fail the benchmark _if the AI notes the reason_. The exclusion does cover "missing step appropriately contraindicated and clearly documented"[\[51\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT2.yaml#L34-L38) - good. The rubric's focus areas mention aspirin, 12-lead, and cath lab alert explicitly[\[52\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/rubrics/A-PRT.yaml#L16-L19), which matches current ACLS/NAEMSP standards for STEMI care (early aspirin and ECG are key interventions[\[25\]](https://www.heart.org/-/media/Files/Affiliates/MWA/MN-ML-EMS-Transport-Guideline.pdf#:~:text=%EF%82%B7%20Administer%20Chewable%20Aspirin%2081,90)[\[50\]](https://www.heart.org/-/media/Files/Affiliates/MWA/MN-ML-EMS-Transport-Guideline.pdf#:~:text=1.%20All%20patients%20with%20non,30%20minutes)). - **DOA pronouncement fields** are mentioned in the rubric focus as well[\[52\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/rubrics/A-PRT.yaml#L16-L19), though those are formally evaluated under A-CMP5. There is conceptual overlap: A-PRT is about protocol-mandated documentation. In many jurisdictions, handling a field pronouncement is protocol-driven (with steps to follow: contact medical control or use standing orders, etc.). The rubric's _critical requirements_ list "DOA must include all four pronouncement elements"[\[53\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/rubrics/A-PRT.yaml#L22-L26), indicating the project considers DOA fields (method, physician, time, agency) part of protocol tracking. In implementation, DOA completeness is A-CMP5, but it's good that A-PRT highlights it too. This ensures it's treated with the same seriousness as RSI and STEMI steps. - **Guideline alignment**: - RSI: The benchmarks are aligned with NAEMSP and other airway management guidelines. For instance, requiring both sedative and paralytic (no "cold intubations" without sedation), and confirmation of placement with EtCO₂, are standard of care[\[42\]](https://www.wvoems.org/media/456430/4903%20-%20rsi%20-%201-2021.pdf#:~:text=%28in%20about%2030%20,high%20intraocular%20pressure%2C%20high%20potassium)[\[54\]](https://www.wvoems.org/media/456430/4903%20-%20rsi%20-%201-2021.pdf#:~:text=5,decreased%20resistance%20to%20ventilation%20within). One might add that documenting number of attempts or who performed intubation is important, but that might be beyond scope. The main critical items are present. We should ensure the prompt or code covers _indication and pre-oxygenation_ as well, since A-PRT1 concept lists those[\[39\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT1.yaml#L10-L18). Indication (why RSI was done) is important to justify it; pre-oxygenation is usually expected (e.g., preoxygenate with 100% O₂ as a step[\[55\]](https://www.wvoems.org/media/456430/4903%20-%20rsi%20-%201-2021.pdf#:~:text=suction%20hooked%20up%2C%20turned%20on%2C,1%29%20of%20the)[\[56\]](https://www.wvoems.org/media/456430/4903%20-%20rsi%20-%201-2021.pdf#:~:text=3,5%20mg%2Fkg)), but if it's missing from output it implies incomplete procedure documentation. Perhaps the AI might miss that if medics didn't explicitly narrate it, but if they did and it's omitted, that's a valid fail. - STEMI: As mentioned, steps align with AHA guidelines. To be thorough, also consider **nitroglycerin and analgesia**: most ACS patients get NTG (if BP okay) and often morphine or fentanyl for pain if needed. These are part of standard treatment (though not as time-critical as ASA). The benchmark doesn't explicitly list them, likely because they might not be absolutely mandatory or might be contraindicated (nitro withheld if low BP or if on ED drugs[\[57\]](https://www.heart.org/-/media/Files/Affiliates/MWA/MN-ML-EMS-Transport-Guideline.pdf#:~:text=in%20the%20past%2024%20hours,90)). Possibly the AI could omit nitro and still pass if ASA and ECG are documented. That's acceptable since nitro might be skipped for good reason. But perhaps ensure if pain was present and no relief med given or explained, that might be flagged under A-PRT2's general "critical steps." This is a bit subjective, so it might be fine leaving it out. The main life-saving interventions are covered (ASA, ECG, hospital notification). - Another protocol that isn't explicitly covered is **Stroke (CVA) alerts**, which we'll address as a gap below. Also **Trauma protocols** (like trauma alerts, tourniquet use, etc.) are not here. Those are candidates for new benchmarks. - **Recommended improvements**: - For RSI (A-PRT1): It looks nearly complete. We might suggest adding an item for **post-intubation care** (e.g., "\[ \] Sedation maintained post-intubation" as a field). If Medic Copilot routinely misses that, perhaps an evaluator or future benchmark could check it. However, this might be beyond current data. - For STEMI (A-PRT2): Ensure **time of first ECG** and **time of hospital alert** are captured if present. The narrative often has "12-lead acquired at 14:35" and "STEMI Alert called to Hospital at 14:40". If Medic Copilot doesn't populate a timestamp or at least notes that these were done, that's an issue. The guidelines emphasize time metrics (first medical contact to device, etc.), and while we may not require those metrics in output, documenting that an alert was made is key. The benchmark already looks for activation/notification[\[45\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT2.yaml#L28-L36), so that's good. Emphasize in the LLM prompt or code that synonyms like "cath lab activated" or "code STEMI called" all count. - **Recommended Changes**: - \[ \] **Add post-RSI sedation note**: If feasible, update A-PRT1's criteria to include whether post-intubation sedation/analgesia is documented when appropriate. For example, after intubation, guidelines advise giving a long-acting sedative or drip to keep the patient comfortable[\[58\]](http://www.miemss.org/home/Portals/0/Docs/Guidelines_Protocols/Protocols_2015_Pilot_Programs.pdf?ver=2015-04-09-133654-283#:~:text=,be%20the%20preferred%20agent)[\[59\]](https://www.emsaware.org/articlesforems/all-about-rsi-a-primer#:~:text=All%20About%20RSI%3A%20A%20Primer,do%20not%20have%20any). If the narrative had it and AI missed it, that's a documentation gap. Even if not a separate benchmark, consider logging it under A-PRT1's concept of "critical fields" (as an extension of RSI care). - \[ \] **Explicitly mention nitro in STEMI prompt**: While A-PRT2 may not fail solely for lack of nitroglycerin (due to possible contraindications), the prompt to the LLM could mention "expected STEMI treatments include ASA (unless allergic) and nitroglycerin (if BP allows)". This way, if the narrative clearly had nitro given and the AI missed it, the LLM might catch it as a discrepancy. Conversely, if nitro was not given because of low BP (and narrative says so, or BP is in data), the AI's omission is justified. This is a subtle point; perhaps making nitro a "softer" check. But including it ensures the AI evaluator at least considers it. - \[ \] **Maintain alignment with protocols**: Periodically review A-PRT benchmarks against updated national guidelines (e.g., new AHA/NAEMSP updates). For instance, if there's a new recommendation for out-of-hospital cardiac arrest care (like mechanical CPR device documentation or checking EtCO₂ during CPR), those could become future protocol checks. Currently, RSI and STEMI are well covered. (We will propose adding Stroke and Cardiac Arrest documentation as new benchmarks in the next section.) - \[ \] **No changes to threshold/weight**: The current 0.85 threshold for A-PRT2 and 0.90 for A-PRT1 seem clinically reasonable. These protocols have some case-by-case variation, so allowing a small margin before failing is okay. We do **not** suggest making them 100%, because occasionally a step might be legitimately skipped. The weights (0.5 each) simply split the rubric evenly; that's fine given these two scenarios are equally critical in different domains.

### A-CMP: Completeness

**Clinical Assessment**: Sound (with Minor Additions)  
**Findings**: - The Completeness rubric ensures all **required DRAATT sections and critical fields** are present in the structured output[\[60\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/rubrics/A-CMP.yaml#L16-L24)[\[61\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/rubrics/A-CMP.yaml#L32-L40). This is fundamental for a useful ePCR: missing a whole section like Treatment or Transport can render a report useless for billing or QA. The benchmarks cover: - **A-CMP1**: Core section presence - checks that Dispatch, Response, Arrival, Assessment, Treatment, Transport/Refusal sections are not entirely missing[\[62\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP1.yaml#L10-L19)[\[63\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP1.yaml#L27-L35). It uses a threshold 0.85 to allow perhaps one section out of six to be missing at most[\[64\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP1.yaml#L20-L25). The design acknowledges some templates legitimately omit sections (e.g., an RSI-focused addendum might not have its own dispatch section)[\[65\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP1.yaml#L32-L40). This is clinically sensible; not every narrative has every section (e.g., a canceled call might lack Arrival/Assessment). The example of missing Response and Transport sections would fail[\[66\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP1.yaml#L37-L44) - rightly so. This broad check ensures no major component is overlooked by the AI. - **A-CMP5**: DOA (Dead On Arrival) pronouncement fields - ensures in cases of obvious death, the four key pronouncement details are present: **pronouncement method** (e.g., by protocol vs. by on-line physician), **pronouncing physician name**, **time of pronouncement**, and **agency** (or authority)[\[67\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP5.yaml#L10-L19)[\[68\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP5.yaml#L27-L35). It has a higher weight (0.167, indicating each completeness sub-check gets equal weight in rubric) and threshold 0.90[\[69\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP5.yaml#L13-L21), meaning missing even one of the four fields drops score to 0.75 or lower (fail). This aligns with common policy: if a field termination of resuscitation is performed, documentation must include who authorized it, when, and under what protocol. Most EMS protocols (and state laws) require exactly these items for pronouncement[\[70\]](https://file.lacounty.gov/SDSInter/dhs/206332_Ref.No.814_DeterminationofDeath_06-21-16.pdf#:~:text=,to%20determine%20death%3B%20the)[\[71\]](https://www.icphd.org/assets/Emergency-Medical-Services/EMS-Policies-Protocols-Procedures-Foms/New-Updates-6.28.24/4110-Termination-of-Resuscitation.pdf#:~:text=,identified%20in%20the%20Advanced). For instance, Los Angeles County mandates documenting _time and criteria of death, and the physician who gave the order_ etc. Medic Copilot must not omit any. The examples given (missing agency, or leaving pronouncement_method null) match this[\[72\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP5.yaml#L36-L39). This benchmark is well-conceived and **clinically complete by itself**. One additional detail that could be considered "critical" in some jurisdictions: documentation of **notification of authorities** (e.g., coroner or police notified). Some systems require that for DOA. The current four fields cover the act of pronouncement; notifying coroner is a subsequent step. Since _agency_ could be interpreted as the agency with jurisdiction (some use "Medical Examiner" as pronouncing agency in certain cases), this might be implicitly covered. However, if not, it's one more element that could be added if aiming for thoroughness (perhaps as an acceptable extra but not required everywhere). - **A-CMP6**: Transport/Handoff completeness - ensures the **Transport or Refusal section** has all needed details for a transported patient: transport mode, any delays/incidents en route, who the patient was turned over to (receiving staff), patient condition on arrival, and disposition of belongings/equipment[\[73\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP6.yaml#L10-L19)[\[74\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP6.yaml#L28-L36). Threshold is only 0.80 here[\[75\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP6.yaml#L14-L22), reflecting that not every transport will have every item (e.g., not every call has belongings to hand off). This is very important for continuity of care and billing. Missing these can cause issues (e.g., not stating to whom care was transferred could raise questions). The inclusion covers both transported and refusal scenarios, and properly excludes cases where no transport was expected (e.g., field pronouncement)[\[74\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP6.yaml#L28-L36). The examples (no receiving provider named, or a refusal with no risks explained) highlight typical omissions[\[76\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP6.yaml#L38-L43) - notably, the refusal example ("no documented explanation of risks") overlaps with our earlier note on refusal documentation. A-CMP6 does hint at refusal documentation ("Transport/Refusal section"), but primarily focuses on transport. There might be room to expand on refusals specifically. - **A-CMP6-airport**: A special case completeness check for **air medical transfer scenarios**[\[77\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP6-airport.yaml#L10-L18). This is a niche but critical scenario: when EMS hands a patient off to an air transport at an airport or helipad, the documentation needs extra fields - which Medic Copilot might easily miss if not explicitly guided. The benchmark ensures inclusion of: **air agency name**, **aircraft tail number**, **destination facility**, **specific diagnosis and reason for transfer**, **source of that diagnosis** (e.g., referring hospital's finding), and **roles of flight vs ground crew** (who was primary caregiver)[\[78\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP6-airport.yaml#L28-L36). Threshold 0.85[\[79\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP6-airport.yaml#L14-L22) means almost all those should be present. This is impressively thorough; it aligns with specialized inter-facility transport documentation practices. Not all EMS systems formally require tail number or crew role documentation, but including it is best practice for traceability. By covering these, Medic Copilot can support continuity between ground and flight providers (and it's great for billing and QI). There are few additional fields one could imagine: perhaps **time of transfer of care** or **mode (rotor vs fixed-wing)**, but those might be extraneous. As is, this covers the major points. The example given (tail number missing, or vague reason given) show why it's needed[\[80\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP6-airport.yaml#L38-L41) - these details often get lost, and the AI should capture them if spoken. - **Jurisdictional standards**: The benchmarks seem to draw from common documentation requirements: - For DOA: Many states require exactly what A-CMP5 checks (time, physician, etc.)[\[71\]](https://www.icphd.org/assets/Emergency-Medical-Services/EMS-Policies-Protocols-Procedures-Foms/New-Updates-6.28.24/4110-Termination-of-Resuscitation.pdf#:~:text=,identified%20in%20the%20Advanced)[\[81\]](https://www.smchealth.org/sites/main/files/file-attachments/507_-_determining_death.pdf?1751406565#:~:text=%5BPDF%5D%20507%20,the%20medical%20examiner%20has). This is complete. Minor addition: consider if _"pronouncement agency"_ could also include cases where the paramedic themselves pronounce under a protocol (then agency might be "per \[County\] EMS Protocol"). The benchmark expects some entry there, which is fine. - For transports: National standards (e.g., NEMSIS data set) include fields for transport mode, delays, condition on arrival, etc. A-CMP6 aligns well with NEMSIS required elements (e.g., transport mode code, destination/hospital code, "patient condition at destination" field, etc.). It looks clinically complete. Perhaps add explicitly _"if patient refused, document that risks/benefits explained and signatures obtained"_ - though the example does mention lack of risk explanation as a failure[\[76\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP6.yaml#L38-L43), indicating that is evaluated. - For air transfers: There isn't a universal standard, but the fields chosen are very relevant. E.g., including **diagnosis source** (was it the sending facility's diagnosis?) helps clarify if the EMS crew is just relaying a hospital's info. **Tail number** is often recorded by flight crews for their records; having it in ground PCR ties the two records together. This is above-and-beyond detail, which is great for completeness in complex transfers. - **Recommended Changes**: - \[ \] **Add coroner notification (jurisdictional)**: If aiming for gold-standard completeness, consider an _A-CMP5b_ or extending A-CMP5 to check if, in DOA cases, a coroner/ME or law enforcement notification is documented when appropriate. For example, some protocols say the paramedic must notify the coroner's office for any field death. If the narrative had "coroner notified" and the AI omitted it, that's a miss. This could be folded into A-CMP5's inclusion (as an optional field): not every system requires it, so it might be an **acceptable deviation** rather than critical requirement. Perhaps: **Recommended** to document notification of authorities, but not failing if absent unless local rules demand it. - \[ \] **Strengthen refusal documentation checks**: The A-CMP6 benchmark touches on refusals (the mention of no documented explanation of risks is crucial). We suggest making that more explicit or a separate sub-benchmark for refusals. Key elements for a refusal are: _decision-making capacity assessed, risks explained, patient's understanding and agreement, and witness/signature_. Medic Copilot should capture if the medic documented these. Right now, only the "risks explained" piece is explicitly in example[\[76\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP6.yaml#L38-L43). Consider adding a line in inclusion criteria that if disposition is refusal, the output must contain evidence of capacity exam and that the patient understood consequences. Perhaps a new benchmark A-CMP7 or part of A-CMP6 logic. This is a high medicolegal risk area - "Patient refusals are the highest risk encounters in EMS" and providers should **record the discussion of risks** and that the patient had capacity[\[82\]](https://www.ems1.com/legal/articles/should-i-stay-or-should-i-go-ZY5uuUB5BtaC8AiB/#:~:text=,try%20to%20convince%2Freassure%20availability). Ensuring those appear in output is vital. - \[ \] **No major threshold changes**: The current thresholds (80-90% range) reflect a realistic tolerance for minor omissions. Completeness is somewhat subjective; thus 100% requirement might be too strict. For example, A-CMP6 at 0.80 means missing one of five transport details might still pass - that's reasonable (not every call has "belongings" to transfer, etc.). We wouldn't change these thresholds globally. However, one could argue DOA (A-CMP5) should be 1.0 because all four fields are truly mandatory by protocol. It's already 0.90, effectively requiring all or all but one. If we aim for perfection, set it 1.0 so that missing any one of the four fails outright. That said, given weight scoring (0.75 if one missing), it likely fails the rubric anyway. So either is fine; 0.90 is practically strict. - \[ \] **Maintain section keyword mapping**: As content evolves (e.g., new sections like a "Telemedicine" section in future?), ensure A-CMP1 covers it. The mapping file mentions how section keywords drive coverage checks[\[83\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/docs/corpus-validator-mapping.md#L80-L88). The current design appears robust for existing DRAATT sections.

### A-TMP: Temporal Ordering

**Clinical Assessment**: Sound  
**Findings**: - The Temporal Ordering rubric ensures the **chronology of events** in the AI-generated report makes sense relative to the narrative timeline[\[84\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TMP1.yaml#L10-L18). This is important to avoid an output that, for example, lists interventions before assessments or contradicts the sequence of care. There are two benchmarks: - **A-TMP1 (Temporal Ordering Incorrect)**: Checks if any key events are out of order (onset, arrival, treatments, reassessments, transport timing)[\[84\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TMP1.yaml#L10-L18)[\[85\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TMP1.yaml#L28-L36). A typical failure would be documentation showing transport left before an on-scene procedure that obviously had to happen earlier, etc. The example given ("transport departure before documented vitals/exam" or "intubation listed before sedative given") matches such errors[\[86\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TMP1.yaml#L38-L41). The threshold is 0.85[\[87\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TMP1.yaml#L14-L22), meaning a small number of minor ordering issues might be tolerated, but systematic disorder is not. This is fair - sometimes documentation might slightly shuffle non-critical details, but major logical sequence errors should be caught. The LLM prompt likely looks at pairs of events to see if any are inverted. Clinically, this is mostly about coherence rather than a specific medical guideline, but it does affect interpretation (e.g., documenting a procedure without any prior indication can confuse readers). - **A-TMP2 (Pre/Post Trend Misinterpreted)**: Focuses on trends around interventions - e.g., did the patient improve after treatment or worsen, and is it recorded correctly?[\[88\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TMP2.yaml#L10-L18). This ensures the AI correctly represents things like "pain improved from 8 to 4 after nitro" rather than accidentally stating pain worsened. Examples: recording a trend backwards (saying a parameter got worse when it actually got better in narrative), or missing the trend entirely[\[89\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TMP2.yaml#L36-L39). Threshold 0.85 again[\[90\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TMP2.yaml#L13-L21). This is clinically significant: misreporting a response to treatment could lead to wrong conclusions (e.g., thinking a medication failed when it succeeded). The inclusion criteria cover swapped or fabricated trends[\[91\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TMP2.yaml#L27-L35). This is a smart check, because LLMs might sometimes jumble before/after if not carefully guided. - These benchmarks are more about internal consistency and logical flow. They don't map to an external clinical standard but rather to the medic's timeline. - **Edge cases**: The rubric properly excludes cases where timeline is genuinely ambiguous or when minor timestamp offsets occur[\[92\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TMP1.yaml#L34-L37). It also excludes when source didn't have pre/post values - you can't fault AI for missing a trend if none was documented[\[93\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TMP2.yaml#L29-L35). This is good. - The LLM prompts (temporal_order_prompt.md and trend_prompt.md) need to encode clinical logic, which is tricky. E.g., the temporal order prompt should "know" that you can't transport before you arrived on scene, etc. It likely gives guidance like "if any event is timestamped or described in an order that contradicts real-time sequence, flag it." As long as it's done systematically (perhaps leveraging known sections: Dispatch before Arrival, Arrival before Treatment, etc.), it should work. No major issues noted here. **Recommended Changes**: - \[ \] **Clarify multiple simultaneous events**: One thing to clarify for the LLM (and code) - sometimes medics document things that happen in parallel (e.g., "IV established and medication given while en route"). The AI might list them sequentially, which is fine as long as it doesn't violate logic. We should ensure the evaluation doesn't unfairly flag parallel actions that are documented sequentially. Perhaps include in acceptable deviations: "Tightly coupled or simultaneous actions may be documented in any order as long as no cause-effect logic is broken." Likely not a big issue but worth noting. - \[ \] **Check for time travel**: The code-based part (if any) could simply scan for obvious time regressions if timestamps exist (e.g., an event timestamp earlier than one that was already described as earlier). If Medic Copilot output includes time data, leveraging that in A-TMP1 would strengthen it. This might already be in place via regex from corpus tools. - \[ \] **Reinforce trend context in prompt**: For A-TMP2's LLM prompt, it might help to list typical clinical trend pairs to look for: pain score, level of consciousness, vital signs, neurological deficits (in stroke) pre and post intervention. The current examples cover pain and EtCO₂[\[89\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-TMP2.yaml#L36-L39). Adding "vital signs around interventions like after fluid bolus or after epinephrine" to the prompt examples could broaden the LLM's scope. This ensures it catches if, say, a hypotension improved but AI says "blood pressure dropped" incorrectly. - \[ \] **No threshold/weight changes**: The 0.85 threshold is fine - it demands high accuracy but not absolute. Temporal nuance can be tricky, so some slack is acceptable. Weight is 0.25 each (making rubric weight 0.125 overall). Given temporal consistency is important but perhaps not as critical as safety or factual accuracy, its current weighting seems appropriate.

### A-EVD: Evidence Attribution

**Clinical Assessment**: Sound  
**Findings**: - The Evidence Attribution rubric (A-EVD) checks whether every structured fact in the output is backed by some span of the source narrative, and that the spans truly support those facts[\[94\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-EVD1.yaml#L10-L18). This is crucial for ensuring Medic Copilot isn't hallucinating findings or missing citations. A-EVD1 is the sole benchmark here: - **A-EVD1 (Evidence Span Unsupported or Missing)**: It looks for cases where a field in draatt_json either has **no cited source text** or the cited text doesn't actually mention that fact[\[95\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-EVD1.yaml#L10-L19)[\[96\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-EVD1.yaml#L27-L35). In other words, if the output says the patient has "acute inferior STEMI" as impression, there better be a narrative segment indicating that (or at least ST-elevations). If not, it's an unsupported claim. The threshold is 0.80[\[97\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-EVD1.yaml#L13-L21), acknowledging some small number of minor fields might lack explicit evidence, but the majority (≥80%) must be supported. Weight is 0.50 (since this rubric has just one benchmark, it carries the full 12.5% rubric weight)[\[98\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-EVD1.yaml#L13-L19). - This is aligned with good clinical documentation practice: any data the system outputs should be traceable to the medic's actual report (especially for audits or legal review). It's essentially measuring completeness of references. It doesn't correspond to a traditional medical guideline but rather a documentation integrity principle. - The examples given are excellent: impression says STEMI but the linked evidence text only says "nonspecific ST changes" (meaning the AI possibly upgraded a finding without basis)[\[99\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-EVD1.yaml#L37-L40), or vital/med fields present but evidence map is empty[\[99\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-EVD1.yaml#L37-L40). Both are serious issues - the first is a false inference, the second is an omission of traceability. - **Nuances**: The exclusion criteria smartly note that if evidence spans are present but just differ in length or include extra words, that's fine[\[100\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-EVD1.yaml#L32-L35). So the evaluation isn't nitpicky about exact span boundaries, just presence and content relevance. - The LLM prompt (evidence_prompt.md) presumably instructs the model to verify if each key element has some quote backing it. This is somewhat technical for an LLM, but presumably it's given the trace with evidence tags. As long as the prompt focuses on clinically relevant facts (vitals, meds, dx, major treatments) and not trivial fields, it should catch most issues. - Given this is a "last line of defense" to catch hallucinations, the relatively lower threshold (80%) could maybe be higher. Ideally, we want nearly all facts supported. However, perhaps some minor fields (like maybe a slightly inferred time or something) might slide. **Recommended Changes**: - \[ \] **Raise threshold modestly**: Consider increasing A-EVD1 threshold to 0.90. In a perfect system, _every_ extracted fact should have evidence. Allowing 20% to lack support is a bit high. If we raise it to 90%, we're saying only very few minor things can go unsupported. For example, if Medic Copilot adds a reasonable inference like "patient transport mode: ground" when the narrative didn't state "by ground" explicitly (because it's implied), that might be unsupported textually but not clinically wrong. Those few cases might be acceptable. But anything more widespread is not. A tighter threshold enforces that principle. - \[ \] **List critical fields to verify**: Ensure the evidence-checking prompt or logic prioritizes clinically significant fields - impressions, medications, procedures, vitals, chief complaint, etc. It's okay if some administrative fields (like "unit number" or similar) might not have explicit evidence in narrative (maybe only in metadata). We don't want to fail the entire eval on something trivial. If not already, calibrate the evaluation to focus on patient care data. Possibly already handled via schema (it likely ignores fields like timestamps). - \[ \] **Incorporate contradiction cross-check**: There is overlap between negation/contradiction and evidence attribution. E.g., if two evidence spans contradict each other (one says X present, another says no X), A-NEG4 handles that. But one might say evidence attribution should also consider if the evidence provided actually _supports_ the output or if it actually _refutes_ it. The example of STEMI impression backed by "non-specific ST changes"[\[99\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-EVD1.yaml#L37-L40) is exactly that - the span is present, but it _refutes_ the impression rather than supports. The benchmark text covers that ("span... fails to mention the fact it supposedly supports"[\[101\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-EVD1.yaml#L28-L35)). We might emphasize in the prompt: "If the linked evidence text does not actually confirm the field (or even contradicts it), that counts as unsupported." This is likely already the intent, but making it explicit helps the LLM judge to catch subtle mismatches, not just missing spans. - \[ \] **Periodically review mapping**: As Medic Copilot evolves to maybe incorporate external knowledge (e.g., auto-filling a protocol step that wasn't narrated), any such feature would need re-evaluation of evidence requirement. For now, requiring direct evidence for everything is appropriate given it's a documentation assistant, not an inference engine.

### A-FMT: Format Validity

**Clinical Assessment**: Sound  
**Findings**: - The Format Validity rubric (A-FMT) ensures the output JSON is syntactically correct and meets schema requirements[\[102\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FMT1.yaml#L9-L17). There is only one benchmark: - **A-FMT1 (DRAATT JSON Format Invalid)**: It checks that draatt_json is present, well-formed, and not containing extra free-text that doesn't belong[\[102\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FMT1.yaml#L9-L17)[\[103\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FMT1.yaml#L26-L34). Essentially, is the output a proper JSON with all required keys for that call type. This is more an engineering check than medical, but extremely important - an improperly formatted report can't be used at all. The threshold is set high at 0.95[\[104\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FMT1.yaml#L13-L21), implying we expect near-perfect adherence, with maybe allowance for a minor schema deviation. Weight 0.50 (the whole rubric, 12.5% of total)[\[105\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FMT1.yaml#L13-L20). - This benchmark draws directly from the synthetic corpus schema validations (frontmatter validation reused)[\[106\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/docs/corpus-validator-mapping.md#L70-L78). It's basically ensuring no output is missing core fields or has invalid enumerations, etc. - Clinically, one might argue this is **critical** because a JSON parse failure or major schema error could mean lost data. They set passing threshold at 95% - likely meaning, for example, if one required field is missing or an enum is slightly off, they don't immediately fail if it's still parseable. But anything more and it fails (score drops to 0.5 or 0). - Example given: output that includes extraneous explanatory text outside JSON, or missing major keys like chief_complaint and vitals[\[107\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FMT1.yaml#L35-L39) - those would fail. - This is straightforward and _should be non-negotiable_: we want valid JSON always. The acceptable deviation might be for minor things like an out-of-range enum value (which is still parseable JSON), hence threshold 0.95 (meaning maybe one minor schema issue allowed). **Recommended Changes**: - \[ \] **Consider requiring 100% format compliance**: Since format errors are objectively preventable and critical, making A-FMT1 threshold 1.0 (never event) is worth considering. A broken JSON or missing core fields is a show-stopper for down-stream systems. We likely don't want to pass any case that isn't proper JSON. The only slight caveat: if Medic Copilot occasionally includes an extra top-level field or a harmless warning string, one might argue the JSON is still parseable. But ideally, even that should be discouraged. Setting threshold 1.0 emphasizes that **the output must be valid and schema-complete every time**. - \[ \] **Update format schema with new fields**: If other recommendations (like new fields for pediatric dosing or adding refusal witness) are implemented, ensure the format validator updates accordingly so that those fields are expected when relevant. The rubric currently aligns with the known schema v1. If the schema evolves (say to include a refusal object with subfields for risks_explained, witness_name, etc.), A-FMT1 must check those too. - \[ \] **No change in weight**: This rubric's weight (12.5%) is fine. It means a format failure significantly drags down the overall score (and likely fails the product if scoring strictly). That's appropriate because format validity is a baseline requirement for any use of the data.

_(The remaining rubrics not explicitly asked about - A-TMP, A-EVD, A-FMT - have been included above for completeness. They generally focus on internal consistency and data integrity rather than clinical content, and as reviewed, they are largely sound with minor tweaks.)_

## Missing Failure Modes (Proposed New Benchmarks)

Despite covering many categories, the current 20 benchmarks miss several **clinically significant documentation errors** that Medic Copilot could make. We propose new rubrics to target these gaps, each with one or more benchmarks addressing specific failure modes. The following are priority areas to add:

### Proposed: A-PED (Pediatric-Specific Documentation)

**Clinical Rationale**: Pediatric calls pose unique documentation challenges that differ from adult cases. Children require weight-based drug dosing, age-specific normals, and special equipment sizing. Errors in these areas can lead to dangerous dosing mistakes or missing critical info like weight. EMS narratives often include Broselow tape color or an estimated weight, as well as notes on parent/guardian interactions. None of the current benchmarks explicitly ensure Medic Copilot handles these pediatric details. Given that **medication dosing accuracy is paramount to safety when treating children**[\[12\]](https://www.chawisconsin.org/wisconsin-emergency-medical-services-for-children-weighs-in-on-improving-pediatric-care-free-broselow-compatible-weight-measuring-tapes-for-all-emergency-medical-service-agencies/#:~:text=%E2%80%9CMedication%20dosing%20accuracy%20is%20paramount,%E2%80%9D) and that _all pediatric meds are dosed by weight_[\[11\]](https://www.chawisconsin.org/wisconsin-emergency-medical-services-for-children-weighs-in-on-improving-pediatric-care-free-broselow-compatible-weight-measuring-tapes-for-all-emergency-medical-service-agencies/#:~:text=administration%2C%20as%20all%20pediatric%20medications,the%20highly%20charged%20clinical%20situations), the system should be evaluated on these factors. This rubric would ensure the AI doesn't overlook pediatric context. **Suggested Benchmarks**: - **A-PED1: Pediatric Dosing Error** - _Medic Copilot outputs a medication dose for a pediatric patient that is inconsistent with the patient's weight or age, or fails to convert a weight-based dose properly._ For example, if the narrative says "Epinephrine 0.01 mg/kg for 20 kg child (~0.2 mg)" and the AI records "0.01 mg" (interpreting it literally) or "2 mg" (decimal error), that's a fail. Also, if the AI suggests a dose that would exceed pediatric safe range despite narrative (e.g., gives an adult dose), that's caught. **Threshold** should be very high (0.95 or 1.0) since such errors are critical. **Justification**: Children are very susceptible to dosing errors; a 10-fold error can be fatal. This complements A-SFT1 (which covers dose safety broadly) by zooming in on weight-calculation mistakes and ensuring use of weight info. It effectively checks if the AI correctly applied the formula or at least didn't introduce a new error in conversion. _(Related guideline: use of tools like Broselow tape to reduce calc errors_[_\[108\]_](https://www.chawisconsin.org/wisconsin-emergency-medical-services-for-children-weighs-in-on-improving-pediatric-care-free-broselow-compatible-weight-measuring-tapes-for-all-emergency-medical-service-agencies/#:~:text=,sizes%20for%20their%20pediatric)[_\[109\]_](https://psnet.ahrq.gov/issue/emergency-medical-services-system-changes-reduce-pediatric-epinephrine-dosing-errors#:~:text=Emergency%20medical%20services%20system%20changes,errors%20in%20emergency%20medical%20services)_; EMS QI studies show documenting weight in kg reduces dosing errors_[_\[109\]_](https://psnet.ahrq.gov/issue/emergency-medical-services-system-changes-reduce-pediatric-epinephrine-dosing-errors#:~:text=Emergency%20medical%20services%20system%20changes,errors%20in%20emergency%20medical%20services)_.)_ - **A-PED2: Pediatric Assessment Completeness** - _Medic Copilot fails to include age-appropriate assessment details or misrepresents developmental status for pediatric patients._ This could include missing a documented **weight** or age in output, omitting an APGAR score for a newborn, or not capturing a Broselow color that was stated. Another example: not documenting the caregiver's consent or refusal in a situation where that's relevant (like treating a minor, where typically you'd note parent gave consent or not). **Threshold** ~0.90. **Justification**: Certain fields are critical in peds cases: weight (as discussed), and if a baby is delivered, APGAR at 1 and 5 minutes is usually expected in narrative. If the medic gives those and the AI drops them, that's a serious omission for neonatal care continuity. (APGAR could be its own benchmark, but we include under pediatric rubric as it's newborn-specific.) Also, GCS for small kids or pediatric-specific exam findings (like cap refill, which is often charted for kids) should ideally appear if mentioned. This benchmark ensures the AI doesn't treat a pediatric call with an adult template, thereby missing key info. It complements completeness and fact extraction for pediatric contexts.

_(If needed, A-PED rubric could have more benchmarks, e.g., one specifically for Broselow/equipment sizing if narratives contain that, but the above two cover dosing and general completeness.)_

### Proposed: A-CAR (Cardiac Arrest Documentation)

**Clinical Rationale**: Out-of-hospital cardiac arrest (OHCA) calls generate complex, timeline-critical data: initial rhythm, times of shocks, drugs, Return of Spontaneous Circulation (ROSC) or termination, etc. These are vital for both clinical review and research, and are often scrutinized by medical directors. Medic Copilot must capture these events accurately (or flag them if missing). Currently, no benchmark focuses on cardiac arrest specifically. The **Safety Flags** rubric touches on post-intervention vitals (A-SFT4), but that's just one aspect. We need checks that the AI documents the arrest event fully. Missing documentation of a ROSC, for example, is huge - it affects outcome measures and hospital handoff. Likewise, not noting the final rhythm or that CPR was in progress could be a critical omission. **Suggested Benchmarks**: - **A-CAR1: ROSC/Termination Not Documented** - _Medic Copilot fails to record the outcome of a cardiac arrest (no ROSC time or declaration of death) when the narrative indicates one occurred._ For instance, if medics state "After 20 minutes of CPR, pulses regained at 08:32" or "no ROSC, called by online medical control at 08:50," the AI must include that as a timestamped event or conclusion (in appropriate sections, maybe Assessment or Transport). If the AI output does not clearly state whether the patient achieved ROSC or was pronounced in field, that's a fail. **Threshold** high (0.95). **Justification**: Knowing whether the patient lived (got pulses back) or died is perhaps the most critical outcome of the call. Not documenting it is a serious oversight. Many EMS forms have a specific field for ROSC time and whether patient was transported or pronounced; the AI should fill those if the info is present. We want essentially 100% capture of this when available. (Guideline: Utstein template for OHCA emphasizes recording ROSC times and outcome of arrest). - **A-CAR2: CPR/Arrest Event Details Missing** - _Medic Copilot omits other key resuscitation sequence details such as initial rhythm, number of shocks delivered, defibrillation times, epinephrine doses given during the arrest, or CPR intervals._ For example, if the narrative says "Initial rhythm VFib, shocked x3, epinephrine given x2, airway placed, ROSC after 10 minutes," and the AI output is vague like "Cardiac arrest, CPR performed" without specifics, that's a fail. This benchmark would look for the presence of at least: initial rhythm, at least one defibrillation or shock if mentioned, and the drugs given in arrest (epi, amiodarone, etc.), as well as any notable events like "advanced airway placed." **Threshold** maybe 0.85 (some lenience if one minor detail missing, but major ones must be there). **Justification**: Clinically, for every arrest we want documentation of the rhythm and interventions timeline. If Medic Copilot summarizes too loosely, important info could be lost. For instance, not noting that 3 shocks were delivered could lead a reviewer to think maybe the patient was never defibrillated. This overlaps with temporal ordering (the sequence of interventions) and fact extraction (the meds given), but framing it as a scenario-specific completeness check ensures the AI captures the full picture of the resuscitation. It's akin to A-PRT2 for STEMI, but for cardiac arrest protocols (which often have mandated data reporting). Many systems require Utstein data elements in the ePCR; this would enforce capturing those.

_(These two could be combined into one benchmark if needed: "Incomplete Cardiac Arrest Documentation," but splitting outcome vs process might be clearer.)_

### Proposed: A-STR (Stroke Documentation)

**Clinical Rationale**: Stroke (especially large vessel occlusion strokes) are time-sensitive like STEMI, and EMS has specific scales and data to document: e.g., last known well (LKW) time, stroke severity scores (FAST, CPSS, LAMS, RACE or NIHSS components), and stroke alert activation to hospital. None of the current benchmarks explicitly verify these. If Medic Copilot misses a documented LKW time or a stroke scale score, it could hinder acute care decisions (e.g., tPA eligibility windows). It is important the AI capture these details when present. **Suggested Benchmarks**: - **A-STR1: Stroke Scale/Ops Omission** - _Medic Copilot fails to include a documented stroke assessment score or exam finding._ For example, if narrative says "LAMS score 4" or "facial droop and arm drift present, speech slurred (Cincinnati positive)", the AI output should reflect those findings or at least that a stroke scale was positive. If the medics gave a numeric score (LAMS, RACE, NIHSS partial) and the AI drops it, that's a fail. Similarly, if the medic describes those neuro exam findings, AI should not omit them. **Threshold** ~0.90. **Justification**: Stroke scales are now an integral part of prehospital stroke care to determine stroke severity and destination. Missing them can be clinically significant. We allow minor phrasing differences, but the content must be there. - **A-STR2: Last Known Well or Stroke Alert Missing** - _Medic Copilot output does not record the "last known well" timestamp or the hospital pre-notification in a stroke case when the narrative contained that info._ Medics usually establish when the patient was last normal (e.g., "Last known well at 14:25 per family") - critical for tPA/thrombectomy decisions. If that's said and AI doesn't put it in the Dispatch or Arrival/Assessment section, that's a gap. Also, if medics say "Stroke alert called to receiving hospital," the AI should capture that in Transport notes. Omission of an alert is similar to missing a STEMI alert (like we check in A-PRT2). **Threshold** high (0.95) for LKW - that's a single data point, must be there, and moderate (0.85-0.9) for hospital alert (since perhaps if multiple alerts in narrative, could confuse the AI, but mostly should get it). **Justification**: _Time last known well_ is possibly the single most important piece of info in stroke management - it defines eligibility for therapy. Not carrying that over is a critical documentation failure. Pre-notification (Stroke Alert) is also key for speeding care; if medics did it, it should be noted. This mirrors the STEMI alert documentation idea[\[49\]](https://www.heart.org/-/media/Files/Affiliates/MWA/MN-ML-EMS-Transport-Guideline.pdf#:~:text=%EF%83%BC%20Document%20Date%20and%20Time,a%20STEMI%20receiving%20center%2C%20receive) but for stroke.

_(If needed, could combine into one "Incomplete Stroke Documentation" benchmark that checks both presence of a stroke exam score and LKW time, etc., but enumerating helps clarity.)_

### Proposed: A-TRM (Trauma Documentation)

**Clinical Rationale**: Major trauma patients require documentation of certain assessments and interventions that might be unique to trauma. Examples: **Glasgow Coma Scale (GCS)** components for head injuries, mechanism of injury details (e.g., fall height, vehicle speed), and use of **hemorrhage control measures** like tourniquets. If Medic Copilot fails to capture a GCS or a tourniquet application mentioned in narrative, that's a serious omission (affecting clinical understanding and potentially billing for procedures). The current benchmarks don't specifically check for these, aside from general completeness. **Suggested Benchmarks**: - **A-TRM1: GCS/Neuro Assessment Missing** - _Medic Copilot does not record a Glasgow Coma Scale or mental status that was documented for a trauma patient._ Many EMS narratives include "GCS 13" or a breakdown (E4 V4 M5) for trauma (especially if head injury or altered). If provided and the AI omits it, that's a fail. Also, if a trauma patient's level of consciousness is described (e.g., "patient confused, eyes opening to voice, localizes pain"), the AI should either output the GCS or at least those descriptors. **Threshold** ~0.90. **Justification**: GCS is a standard data point for trauma triage and is needed for trauma center criteria. A missing GCS could mean loss of critical info for the receiving ED (e.g., trend of GCS). This benchmark ensures AI doesn't skip numeric scores or detailed neuro exam info for trauma. - **A-TRM2: Critical Trauma Intervention Omitted** - _Medic Copilot fails to document a major trauma intervention that was done as per narrative._ Examples: tourniquet application, wound packing, needle decompression, pelvic binder application. If the narrative says the crew placed a tourniquet on the left leg at 12:00, the AI output must include that in Treatment. If it doesn't, that's a fail. Similarly for chest decompression in a suspected tension pneumothorax - a big procedure that must appear. **Threshold** high (0.95) because these are life-saving procedures that absolutely should be documented. **Justification**: Missing these makes the record dangerously incomplete - e.g., a hospital team not seeing a tourniquet documented might unwittingly leave it on too long or not realize one was used. Also, trauma systems track these interventions (for performance improvement). We want Medic Copilot nearly perfect in recording any such procedure that the medics perform. - _(Optional)_ **A-TRM3: Mechanism/Scene Details Missing** - _If the narrative described critical scene information (e.g., "ejection from vehicle, 10ft fall, extrication time"), and the AI fails to include any mention in the structured output_, that's a miss. This might be harder to quantify in a benchmark, as mechanism is often narrative text that might not neatly map to a structured field except as part of narrative summary or dispatch notes. But it could be part of completeness: the Dispatch/Response sections should include scene details. If Medic Copilot has a field for mechanism or it could include it in narrative summary, it should. This could be a lower weight benchmark if implemented. However, it might overlap with A-CMP1 (ensuring Dispatch section covers scene). We can keep this as a consideration.

### Proposed: A-OBS (Obstetric & Neonatal Documentation)

**Clinical Rationale**: In OB emergencies (childbirth, pregnancy complications), there are unique documentation needs: **APGAR scores** for newborns, **delivery times**, details of any complications (nuchal cord, etc.), and pregnancy specifics (gravida/para, gestational age). Medic Copilot should capture these when present. For example, if a baby is delivered, medics usually note APGAR at 1 and 5 minutes. If the AI misses those, pediatric care might suffer (hospital pediatric team wants that info). Additionally, if a mother is pregnant with, say, 36 weeks gestation and has complications, that gestational age is vital context. This rubric covers errors around birth events and pregnancy. **Suggested Benchmarks**: - **A-OBS1: APGAR/Neonatal Info Missing** - _Medic Copilot fails to document newborn status after field delivery._ If the narrative provides APGAR scores or describes the newborn's condition ("baby boy, crying, APGAR 8 and 9"), the AI output must include that in the newborn's assessment. If not, fail. Also, if any neonatal resuscitation steps were done (stimulation, oxygen, etc.) and not recorded, that's included. **Threshold** 0.95. **Justification**: APGAR is a standard measure for newborns; omitting it is a significant gap in a delivery case. It's often reported to the receiving hospital's neonatal team. - **A-OBS2: Pregnancy Details Omitted** - _Medic Copilot omits critical maternal details in an obstetric case._ For instance, failing to mention gestational age of the pregnancy, or not recording that this is a multiple gestation (twins) if stated, or not noting complications like bleeding or hypertension that were in narrative. Also, if mother's Gravida/Para or prenatal care info was given by medics, the AI should capture it in history. **Threshold** ~0.90. **Justification**: These details set context for OB emergencies. Gestational age (weeks) guides treatment (e.g., 24 weeks vs 38 weeks is very different in viability). If medics verbalize it, the report should reflect it. This benchmark ensures the AI doesn't produce a bland account like "Pregnant patient in labor, transported" missing that she's e.g. 35 weeks with possible preeclampsia. - Possibly also include: **delivery time omission** - if baby was born at a certain time and AI missed that timestamp, that's important (but maybe minor enough to include under APGAR benchmark or overall completeness).

### Proposed: A-BHV (Psychiatric/Behavioral Documentation)

**Clinical Rationale**: Behavioral emergencies (psychiatric crises, violent patients, etc.) require documentation of patient's decision-making capacity, any use of restraints (physical or chemical), and often law enforcement involvement. Medic Copilot could err by not noting that the patient was restrained or that sedation was given for agitation, even if the medic described it. Such omissions can have legal ramifications (restraints usage needs thorough documentation to defend EMS actions). Also, capacity assessment (as discussed in refusals) is key if a patient refuses transport or is placed on psychiatric hold. **Suggested Benchmarks**: - **A-BHV1: Restraint/Sedation Omission** - _Medic Copilot fails to document that restraints were applied or sedatives given in a combative patient scenario._ If narrative says "patient was combative, secured to stretcher with soft restraints" or "received 5 mg Versed for agitation," the AI must include that in Treatment and/or Narrative. If it doesn't, that's a fail. **Threshold** 0.95. **Justification**: Restraint use is a high-liability event; it must be clearly documented for patient safety and legal protection. Missing it could be extremely problematic (e.g., injuries could occur and the record wouldn't show why or how patient was controlled). Similarly, giving a sedative like benzodiazepines or ketamine for agitation is a big intervention - if the AI misses that med, it's also caught by A-FCT1 perhaps, but we highlight it here because it's scenario-specific. Basically, the AI should never omit a restraint or chemical sedation that was performed. - **A-BHV2: Capacity/Consent Not Documented** - _Medic Copilot does not record the assessment of decision-making capacity or the fact that patient consent (or lack thereof) in a behavioral case, when such context was provided._ For instance, if narrative says "patient deemed not competent due to psychosis for refusing treatment, transported on a 5150 hold" or "patient with normal decision-making who consents to treatment after explanation," the output should reflect that (perhaps in the Transport or Narrative summary). If the AI leaves out that the patient was on a hold or that they were incompetent, it fails. **Threshold** ~0.90. **Justification**: Documenting that a patient lacked capacity justifies treating or transporting against their will; documenting they had capacity and refused is critical for liability. This ties into refusal documentation as well but applies generally in psych calls where sometimes patient cooperation is an issue. We want the AI to capture statements about mental status as it pertains to consent. This benchmark ensures Medic Copilot preserves those medico-legal details. (As per best practices, EMS providers should _document that they explained risks and assessed capacity_[\[110\]](https://www.ems1.com/legal/articles/should-i-stay-or-should-i-go-ZY5uuUB5BtaC8AiB/#:~:text=To%20have%20capacity%3A)[\[82\]](https://www.ems1.com/legal/articles/should-i-stay-or-should-i-go-ZY5uuUB5BtaC8AiB/#:~:text=,try%20to%20convince%2Freassure%20availability), so the AI should maintain such notes.) - _(Note: A lot of capacity/refusal overlap exists; if A-REF rubric is made, some of this might go there. But behavioral cases often involve implied consent or holds, even if not formally "refusal," so it's worth covering here too.)_

### Proposed: A-REF (Refusal Documentation)

**Clinical Rationale**: When a patient refuses transport or treatment, EMS must document elements like capacity, what was explained (risks/benefits), that the patient understood and still refused, and that they signed a release (or a witness did). This is one of the highest risk documentation areas because lawsuits often hinge on what was told to the patient who refused. While A-CMP6 and our A-BHV2 suggestion touch on this, a dedicated rubric highlights its importance. Medic Copilot should excel at capturing refusal dialogs because they're often somewhat formulaic in narratives (many EMS providers use standardized language). **Suggested Benchmarks**: - **A-REF1: Inadequate Refusal Explanation** - _Medic Copilot's output for a refusal case does not include that risks of refusal were explained and/or the patient's reasons for refusal._ If narrative states "Advised patient of risks including death without treatment, patient still refuses" and the AI output simply says "Patient refused transport" with no detail, that's a fail. **Threshold** 1.0 (really this should always be documented). **Justification**: It's crucial to show that the medic did their due diligence. **Explain real risks** to the patient and **record that discussion** are standard of care in refusals[\[82\]](https://www.ems1.com/legal/articles/should-i-stay-or-should-i-go-ZY5uuUB5BtaC8AiB/#:~:text=,try%20to%20convince%2Freassure%20availability). The AI should not omit such statements when they exist in the transcript. This benchmark ensures the AI includes a line akin to "Patient advised of potential complications up to and including death, and still refuses." - **A-REF2: Refusal Signature/Witness Missing** - _Medic Copilot fails to record the presence or absence of a refusal signature or witness when applicable._ Many narratives end with "Patient signed refusal form" or "patient refused to sign, witness John Doe". If that's spoken, the output should capture it (perhaps in Transport or a dedicated refusal section). If it's absent, the AI might not infer it, but if medic explicitly says it and AI misses it, fail. **Threshold** 0.85 (some medics might not narrate this clearly, but if they do, want it). **Justification**: This is a legal document aspect - showing that paperwork was done. It might be considered slightly less "medical" but it's part of a complete refusal documentation. Even if not used for AI decisions, including it can protect providers later. - (If only one benchmark is desired for refusals, combine into "Refusal documentation incomplete" that checks for mention of risks discussed and signature/witness. But having two allows weighting if needed.)

Each of these proposed rubrics/benchmarks addresses a gap where the AI could otherwise silently make a significant error. They should be prioritized based on frequency and risk: Pediatric dosing (A-PED1) and Cardiac Arrest outcome (A-CAR1) likely top priority due to safety implications; Refusal documentation errors are high risk legally; Stroke and Trauma details are next for critical clinical handoff info.

## Threshold and Weight Recommendations

Below is a summary table of specific benchmarks where adjusting the passing threshold or weight is advised to better reflect clinical criticality:

| Benchmark | Current Threshold | Recommended | Clinical Justification |
| --- | --- | --- | --- |
| **A-SFT1** (Dose Safety) | 0.95 | **1.0** (never event) | No unsafe dose or route is acceptable. Even one overdose can be catastrophic; thus any violation should fail[\[111\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT1.yaml#L22-L25). Setting 1.0 signals zero tolerance (the rubric's MIN logic already treats any fail as critical). |
| **A-SFT2** (Contraindications) | 0.95 | **1.0** (never event) | Similarly, administering a medication despite a known severe allergy or contraindication must never be missed[\[112\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-SFT2.yaml#L22-L30). This warrants a 100% standard - these are bright-line safety errors with medicolegal implications. |
| **A-FMT1** (Format Validity) | 0.95 | **1.0** (never event) | The output must be valid JSON and include all core sections every single time[\[102\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FMT1.yaml#L9-L17)[\[113\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FMT1.yaml#L21-L25). A non-parseable or severely malformed report is unusable. Minor schema deviations should ideally be fixed rather than tolerated. |
| **A-CMP5** (DOA Fields) | 0.90 | 1.0 (critical fields) | Pronouncement method, time, physician, agency are mandated by protocol in field death cases[\[67\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP5.yaml#L10-L19)[\[68\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP5.yaml#L27-L35). Missing even one can cause legal issues (e.g., "who pronounced?"). This is essentially a checklist that must be fully complete. |
| **A-EVD1** (Evidence Support) | 0.80 | 0.90 | At least 90% of structured facts should have supporting narrative evidence[\[114\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-EVD1.yaml#L20-L28). We want to tighten this to catch more unsupported outputs. An AI documentation assistant should very rarely introduce facts without basis. The slight allowance (10%) is only for trivial items or implicit info. |
| **A-PRT1** (RSI Fields) | 0.90 | 0.90 (no change) | (Already high; no change) All critical RSI steps should be present[\[115\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT1.yaml#L22-L30). We keep at 90% since one minor detail (e.g., forgetting to mention "preoxygenation") might be tolerated, but sedative, paralytic, ETT, and confirmation must be there. |
| **A-PRT2** (STEMI Steps) | 0.85 | 0.85 (no change) | (No change) Allows flexibility if one step wasn't done due to legitimate reasons[\[45\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-PRT2.yaml#L28-L36). E.g., if aspirin withheld for allergy (and documented), it shouldn't fail. 85% ensures most steps are there unless truly contraindicated. |
| **A-CMP6-airport** (Air transfer) | 0.85 | 0.85 (no change) | (No change) Already fairly strict for a special scenario. Given many fields (up to ~7 items[\[78\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-CMP6-airport.yaml#L28-L36)), 0.85 means maybe one can be missing. That's reasonable because sometimes one piece (like exact tail number) might not be caught by AI if not explicitly stated. |
| _(Other benchmarks)_ | _(various)_ | _Review in future_ | Many other thresholds (0.80-0.90 range) seem appropriate given their domain (e.g., Negation at 0.85-0.90, Fact Extraction 0.85-0.90). We recommend focusing on the critical safety and completeness ones above for now. |

We do not see a need to adjust rubric-level weights at this time, except possibly to emphasize Safety. Currently all rubrics are equal weight (12.5%). If a shift in overall scoring emphasis is desired, one could increase A-SFT's weight (and correspondingly lower others) to reflect that a safety failure should have outsized impact. However, since A-SFT rubric already uses MIN (any fail = rubric fail) and likely any rubric fail = system fail, the practical effect is already strong. Therefore, the main adjustments are raising thresholds to effectively demand perfection in the most critical benchmarks.

## LLM Prompt Improvements

Finally, we assess the LLM-as-Judge prompt files for medical accuracy and completeness. For each prompt, we note any clinical nuance missing and suggest additions:

### **negation_simple_prompt.md**

**Issue**: The prompt currently provides examples of straightforward negations ("denies chest pain", "no shortness of breath") and instructs setting true if a negation was misinterpreted[\[4\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/evaluators/llm-judge/negation_simple_prompt.md#L4-L12). It may not explicitly mention synonyms like "negative for X" or "without X," which are common in EMS narratives. Also, it focuses on presence vs absence but doesn't mention ensuring that an explicit denial should not be left as "unknown." This is implied but not explicit. **Suggested Addition**: _Expand the focus list to include other phrasing equivalents._ For example:

Focus ALSO on phrases like:  
\- "negative for \[symptom\]"  
\- "\[symptom\] not present" or "without \[symptom\]"  
<br/>These should be treated as simple negations as well.

Additionally, add a condition to the criteria: _"…or the output leaves the finding in an indeterminate state ('unknown') despite a clear denial."_ This covers cases where the AI neither marks it true nor false. This will prompt the LLM judge to catch omissions where something was denied but the AI failed to mark it false (which is effectively a misinterpretation by omission).

### **negation_complex_prompt.md**

**Issue**: This prompt covers double negatives and implicit/hedged language. It should handle tricky constructs like "not denying chest pain" or "no other complaints" as well as uncertain statements. One nuance: phrases with "but". E.g., "Patient doesn't deny chest pain but states it's mild" - the "doesn't deny" is a double negative meaning they _have_ chest pain. Also "but" in "denies chest pain **but** has some chest tightness" is actually a contradiction scenario. Possibly the prompt doesn't explicitly mention conjunctions or exceptions. **Suggested Addition**: Include an example or note on exception phrases. For instance:

Focus on indirect or complex negation cues such as:  
\- Double negatives: "not denying X" (implies X may be present)  
\- Implicit negatives: "no other issues" (implies aside from what's mentioned)  
\- Hedged statements: "patient isn't sure if X" or "possibly denies X"  
<br/>Also be cautious of contrast words:  
\- e.g. "denies chest pain but..." - text after "but" may indicate a contradiction or exception.

And update the criteria: "Set {output_field} to true if the output records a fact incorrectly due to misreading such phrases (e.g., it marks a symptom absent when the narrative's phrasing actually implies it is present or uncertain)." This helps the LLM flag cases where, say, "not denying pain" was wrongly turned into pain:false.

### **contradiction_prompt.md**

**Issue**: The contradiction prompt likely instructs the LLM to find incompatible findings[\[116\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-NEG4.yaml#L26-L34). It should emphasize temporal context - the difference between a true contradiction vs a change over time. The YAML exclusion criteria do mention ignoring legitimate temporal changes[\[117\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-NEG4.yaml#L32-L39), but the prompt must make that clear to the model. Another nuance: single-sentence contradictions with "but" or "however" (e.g., "skin warm and dry but patient diaphoretic" in one sentence). We should ensure the LLM checks within sections as well, not just across sections. **Suggested Addition**: Add a line in the scope like:

Ignore findings that change over time (e.g., vital signs improving after treatment). We are concerned with contradictions that cannot both be true at the same time.

And include an example with "but":

Example: "Patient is alert and oriented x4 \*but\* confused." These statements conflict and if both appear in output without clarification, that's a contradiction.

Also, explicitly instruct: _"Check within the same time frame or assessment for directly conflicting descriptors."_ This ensures the LLM looks at simultaneous descriptions.

### **vitals_extraction_prompt.md**

**Issue**: The prompt likely asks if vitals in output match those in narrative[\[118\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FCT1.yaml#L28-L36). It might not mention pediatric normal ranges or unit conversion tolerance. The rubric allowed minor rounding and unit changes[\[119\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/rubrics/A-FCT.yaml#L20-L28), but if the prompt doesn't mention it, the LLM might flag, say, 37°C vs 98.6°F as a discrepancy. **Suggested Addition**:

(Note: Minor rounding or unit conversions are acceptable. For example, 37°C vs 98.6°F should be considered a match.)

This guides the LLM to not penalize correct conversions or trivial differences. Also, if multiple sets of vitals were present, ensure the prompt says to consider each set - maybe it already does by context, but if not, clarify: _"Check all recorded vital signs. If any vital in the output differs from what was said (beyond trivial rounding), that's an error."_

If pediatric, maybe mention "If a vital sign is clearly mis-recorded (e.g., a heart rate of 120 becomes 210 due to a digit error), flag it." (Though that's obvious, the LLM will do it anyway.)

### **impression_mismatch_prompt.md**

**Issue**: This prompt checks if the final impression/diagnosis matches ground truth[\[14\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FCT4.yaml#L10-L19). One nuance: sometimes multiple impressions could be reasonable. The rubric says not to flag if the chosen impression is in the set of reasonable differentials[\[120\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FCT4.yaml#L32-L36). The LLM needs guidance on that clinical nuance. Also synonyms as mentioned earlier. **Suggested Addition**:

Focus only on clear mismatches. If the output impression is clinically equivalent to the expected one (e.g., "MI" vs "heart attack", or "STEMI" vs "acute MI"), that is acceptable and should NOT be flagged.  
If multiple diagnoses could be reasonable (e.g., "acute coronary syndrome" vs "STEMI" when ST-elevations were borderline), do not flag those subtle differences.

And perhaps provide a counter-example:

However, if the output picks a completely different diagnosis (e.g., labels a clear STEMI case as "anxiety"), that is a mismatch.

This will help the LLM not false-positive on wording differences. It aligns with rubric criteria on exclusion[\[120\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/2f00813ebbc305ec351c543ff44a2c638a3ce606/zy_experimental/soar-pydantic-eval/benchmarks/a-component/A-FCT4.yaml#L32-L36).

### **repeat_vitals_prompt.md**

**Issue**: The prompt should capture whether required reassessments took place. It needs to define "high-risk intervention" clearly to the LLM. If it just says "after an intervention," the model might not know which interventions require vitals. Also, ensure it accounts for cases where multiple interventions occurred - it should expect multiple rechecks possibly. **Suggested Addition**:

"High-risk interventions" include things like:  
\- Giving certain medications (e.g., nitroglycerin for chest pain, opioids for pain, sedatives like midazolam/ketamine, paralytics for RSI)  
\- Performing invasive procedures (e.g., intubation, cardioversion)  
After such events, vitals (BP, HR, etc.) or patient status (pain level, sedation level) should be recorded again.

Add to criteria: _"Set output_field to true if an intervention of that nature occurred and the output has no subsequent vital signs or reassessment note,_ _and_ _the output doesn't indicate any reason for omission."_

Additionally,

Ignore if the narrative itself states no reassessment was done due to circumstances (e.g., "no second set of vitals due to immediate patient handoff").

This covers an edge case where medics didn't reassess and they noted why - the AI can't invent vitals, so we wouldn't want to flag that if it was appropriately documented as not done. But usually, not done is a system failure anyway; still, good to distinguish AI omission vs medic omission.

### **labeling_guide.md** (if applicable)

Although not a prompt for the model, if there is a human labeling guide referenced[\[121\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/evaluators/llm-judge/CLAUDE.md#L3-L11)[\[122\]](https://github.com/SimWerx/synthetic-data-epcr-narrative/blob/621aeb464f14deb10d20b306d56383237fee469a/zy_experimental/soar-pydantic-eval/evaluators/llm-judge/CLAUDE.md#L88-L93), ensure it aligns with the above changes so that human and AI evaluators are on the same page.

Each prompt should be reviewed after implementing these changes with a few test cases to confirm that the LLM's judgment aligns with SME expectations.

